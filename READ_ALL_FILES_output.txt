File: README.md
# cautious-palm-tree
File: app\main.py
# app/main.py
from fastapi import FastAPI, Query, WebSocket,Response, status
from typing import AsyncIterable, Dict, Any
from app.api.v1.routers import api_router
from app.core.config import settings
import logging
from pathlib import Path
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from app.services.root_agent.agent import supervisor
import os, json, base64, asyncio
from google.adk.sessions.in_memory_session_service import InMemorySessionService
from google.adk.artifacts import InMemoryArtifactService
from google.adk.runners import Runner
from google.adk.events.event import Event
from google.genai import types
from google.adk.agents import LiveRequestQueue
from google.adk.agents.run_config import RunConfig
from typing import AsyncIterable
from app.services.visualization_agent.agent import _artifact_service, _session_service as _vis_session_service

# Configure basic logging for the FastAPI app
logging.basicConfig(
    level=logging.ERROR, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("fastapi_app")


APP_NAME = "Serena Agent"
session_service = InMemorySessionService()
artifact_service = _artifact_service # Placeholder for artifact service, if needed later

async def start_agent_session(
    session_id: str,
    user_sends_audio: bool, # True if user input can be audio
    client_wants_agent_audio_output: bool # True if client wants audio output from agent
):
    """Starts an agent session"""
    logger.info(
        f"Starting agent session {session_id}. User sends audio: {user_sends_audio}, Client wants agent audio: {client_wants_agent_audio_output}"
    )
    # Create a Session
    session = await session_service.create_session(
        app_name=APP_NAME,
        user_id=session_id,
        session_id=session_id # Using user_id as session_id for simplicity here
    )
    # Create a Runner
    runner = Runner(
        app_name=APP_NAME,
        agent=supervisor, #general_agent, #root_agent, #general_agent, # Ensure general_agent is correctly defined and imported
        session_service=session_service,
        artifact_service=artifact_service
    )

    # Create run config with basic settings
    config: Dict[str, Any] = {}
    response_modalities = []

    if client_wants_agent_audio_output:
        response_modalities.append("AUDIO")
        # Create speech config with voice settings only if agent audio output is desired
        speech_config = types.SpeechConfig(
            voice_config=types.VoiceConfig(
                prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Kore")
            )
        )
        config["speech_config"] = speech_config
        # If client wants agent audio, ADK can provide transcription of that audio
        config["output_audio_transcription"] = {}
        logger.info("Agent configured for AUDIO output with transcription.")
    else:
        # If client does not want agent audio, agent should only respond with text
        response_modalities.append("TEXT")
        logger.info("Agent configured for TEXT output only.")
    
    config["response_modalities"] = response_modalities

    # Configure input audio transcription if the user is sending audio
    if user_sends_audio:
        config["input_audio_transcription"] = {}
        logger.info("Agent configured for input audio transcription.")
    else:
        logger.info("Agent not configured for input audio transcription (user sends text).")

    run_config = RunConfig(**config)
    logger.debug(f"RunConfig: {run_config}")

    # Create a LiveRequestQueue for this session
    live_request_queue = LiveRequestQueue()
    # Start agent session
    live_events = runner.run_live(
        session=session,
        live_request_queue=live_request_queue,
        run_config=run_config,
    )
    return live_events, live_request_queue


async def agent_to_client_messaging(
    websocket: WebSocket, live_events: AsyncIterable[Event | None]
):
    """Agent to client communication. Processes events from ADK and sends to WebSocket client."""
    logger.info(f"Task agent_to_client_messaging started for websocket: {websocket.client}")
    try:
        async for event in live_events:
            if event is None:
                continue

            # --- Optional: Enable for very detailed raw ADK event logging ---
            # logger.debug(f"[ADK_RAW_EVENT]: {event}") 

            # 1. Handle Turn Completion or Interruption
            if event.turn_complete or event.interrupted:
                message: Dict[str, Any] = { 
                    "turn_complete": event.turn_complete,
                    "interrupted": event.interrupted,
                }
                logger.info(f"[AGENT_TO_CLIENT_SEND - TURN_STATUS]: {message}")
                await websocket.send_text(json.dumps(message))
                continue # Move to next event
            # function_responses = event.get_function_responses()
            # if function_responses:
            #     for response in function_responses:
            #         # Check if this is the output from our visualization pipeline
            #         if response.name == "execute_visualization_pipeline":
            #             logger.info(f"Detected tool output from: {response.name}")
            #             try:
            #                 # The result is in `response.response`. It might be a dict or a JSON string.
            #                 result_data = response.response
            #                 if isinstance(result_data, str):
            #                     result_data = json.loads(result_data)
                            
            #                 # Check if an artifact was successfully created
            #                 if result_data.get("artifact_saved") == "plot.png":
            #                     artifact_session_id = result_data.get("session_id")
            #                     artifact_filename = result_data.get("artifact_saved")
                                
            #                     # Construct the URL to the artifact endpoint
            #                     image_url = f"/artifacts/{artifact_session_id}/{artifact_filename}"
                                
            #                     # Send a special message to the client with the image URL
            #                     viz_message = {
            #                         "role": "model",
            #                         "mime_type": "image/png",
            #                         "data": image_url,
            #                         "caption": "Here is the visualization you requested:"
            #                     }
            #                     logger.info(f"[AGENT_TO_CLIENT_SEND - VISUALIZATION]: {viz_message}")
            #                     await websocket.send_text(json.dumps(viz_message))
            #             except (json.JSONDecodeError, KeyError, TypeError) as e:
            #                 logger.error(f"Error parsing visualization tool output: {e}. Output was: {response.response}")
            function_responses = event.get_function_responses()
            if function_responses:
                for response in function_responses:
                    try:
                        result_data = response.response
                        if isinstance(result_data, str):
                            result_data = json.loads(result_data)

                        # GENERIC CHECK: Does this tool response contain artifact info?
                        is_artifact_response = (
                            isinstance(result_data, dict) and
                            result_data.get("artifact_saved") and 
                            result_data.get("artifact_saved") != "No" and
                            result_data.get("session_id") and
                            result_data.get("app_name") # Check for the new key
                        )

                        if is_artifact_response:
                            logger.info(f"Detected tool output with artifact from: {response.name}")
                            
                            app_name = result_data["app_name"]
                            artifact_session_id = result_data["session_id"]
                            artifact_filename = result_data["artifact_saved"]
                            
                            # Construct the NEW, generalized URL to the artifact endpoint
                            image_url = f"/artifacts/{app_name}/{artifact_session_id}/{artifact_filename}"
                            
                            # Send a special message to the client with the image URL
                            artifact_message = {
                                "role": "model",
                                "mime_type": "image/png", # Assuming all artifacts are images for now
                                "data": image_url,
                                "caption": "Here is the content you requested:"
                            }
                            logger.info(f"[AGENT_TO_CLIENT_SEND - ARTIFACT]: {artifact_message}")
                            await websocket.send_text(json.dumps(artifact_message))

                    except (json.JSONDecodeError, KeyError, TypeError) as e:
                        logger.error(f"Error parsing tool output from '{response.name}': {e}. Output was: {response.response}")

            # 2. Handle Function Calls
            calls = event.get_function_calls()
            if calls:
                for call in calls:
                    tool_name = call.name
                    arguments = call.args
                    message: Dict[str, Any] = {
                        "mime_type": "text/plain", 
                        "data": f"Tool Call: {tool_name}, Args: {arguments}",
                        "role": "system", 
                    }
                    logger.info(f"[AGENT_TO_CLIENT_SEND - TOOL_CALL]: {message}")
                    await websocket.send_text(json.dumps(message))
                # If a function call event is also a final response or has other content,
                # it will be processed further. If not, we might want to `continue` here
                # depending on ADK event structure for tool calls. For now, let it pass through.
            
            # 3. Handle Content Parts (Text from User/Model, Audio from Model)
            if event.content and event.content.parts:
                event_content_role = event.content.role 
                # logger.debug(f"[ADK_EVENT_CONTENT_OBJECT] Role: '{event_content_role}', Partial: {event.partial}, #Parts: {len(event.content.parts)}")
                
                for i, part in enumerate(event.content.parts):
                    # Log details of each part for debugging
                    # logger.info(f"[ADK_EVENT_PART_{i}] EventContentRole: '{event_content_role}', PartType: {type(part)}, PartDetails: {part}")

                    if not isinstance(part, types.Part):
                        logger.warning(f"Part {i} is not an instance of types.Part. Skipping.")
                        continue

                    text_message_to_send: Dict[str, Any] | None = None
                    audio_message_to_send: Dict[str, Any] | None = None

                    # A. Check for Text in the part
                    if part.text:
                        if event_content_role == "user": # Transcription of user's speech
                            text_message_to_send = {
                                "mime_type": "text/plain",
                                "data": part.text,
                                "role": "user_transcription",
                            }
                        # --- MODIFIED: Handle model text even if EventContentRole is None ---
                        elif event_content_role == "model" or event_content_role is None:
                            # If it has text and isn't user_transcription, assume it's model's response.
                            # This catches intermediate text chunks with EventContentRole: 'None'.
                            text_message_to_send = {
                                "mime_type": "text/plain",
                                "data": part.text,
                                "role": "model", # Send with role 'model' for client handling
                            }
                        # --- END OF MODIFICATION ---
                    
                    # B. Check for Inline Audio Data (expected for model's audio output)
                    if part.inline_data and \
                       part.inline_data.mime_type and \
                       part.inline_data.mime_type.startswith("audio/pcm"):
                        
                        # Assume PCM audio from this stream is model's output
                        audio_data_bytes = part.inline_data.data
                        if audio_data_bytes:
                            audio_message_to_send = {
                                "mime_type": "audio/pcm",
                                "data": base64.b64encode(audio_data_bytes).decode("ascii"),
                                "role": "model", 
                            }
                            logger.info(f"Prepared audio message (role: model) from part {i} with EventContentRole: '{event_content_role}'")
                        else:
                            logger.warning(f"Part {i} (assumed model audio) has empty audio data. EventContentRole: '{event_content_role}'")


                    # C. Send any messages derived from this part
                    if text_message_to_send:
                        logger.info(f"[AGENT_TO_CLIENT_SEND - TEXT_PART]: Role '{text_message_to_send['role']}', Data: '{text_message_to_send['data'][:50]}...'")
                        await websocket.send_text(json.dumps(text_message_to_send))
                    
                    if audio_message_to_send:
                        logger.info(f"[AGENT_TO_CLIENT_SEND - AUDIO_PART]: Role '{audio_message_to_send['role']}', Mime: '{audio_message_to_send['mime_type']}'") 
                        await websocket.send_text(json.dumps(audio_message_to_send))
            # else:
            #     logger.debug(f"Event received with no content or parts to process for messaging. Author: {getattr(event, 'author', 'N/A')}")

    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected in agent_to_client_messaging: {websocket.client}")
    except asyncio.CancelledError:
        logger.info(f"Task agent_to_client_messaging cancelled for websocket: {websocket.client}")
    except Exception as e:
        logger.error(f"Critical error in agent_to_client_messaging for {websocket.client}: {e}", exc_info=True)
        try:
            # Attempt to send an error message to the client
            await websocket.send_text(json.dumps({"error": str(e), "role": "system"}))
        except Exception as send_err:
            logger.error(f"Failed to send error to client {websocket.client}: {send_err}")
    finally:
        logger.info(f"Task agent_to_client_messaging finished for websocket: {websocket.client}")

async def client_to_agent_messaging(
    websocket: WebSocket, live_request_queue: LiveRequestQueue
):
    """Client to agent communication"""
    while True:
        try: # Added try-except to handle potential disconnects gracefully
            message_json = await websocket.receive_text()
            message = json.loads(message_json)
            mime_type = message["mime_type"]
            data = message["data"]
            role = message.get("role", "user")

            if mime_type == "text/plain":
                content = types.Content(role=role, parts=[types.Part.from_text(text=data)])
                live_request_queue.send_content(content=content)
            elif mime_type == "audio/pcm":
                decoded_data = base64.b64decode(data)
                live_request_queue.send_realtime(
                    types.Blob(data=decoded_data, mime_type=mime_type)
                )
            else:
                logger.error(f"Mime type not supported: {mime_type}")
                # Optionally send error back to client
                await websocket.send_text(json.dumps({"error": f"Mime type not supported: {mime_type}", "role": "system"}))

        except asyncio.CancelledError:
            logger.info("client_to_agent_messaging task cancelled.")
            break
        except WebSocketDisconnect: # Specific exception for WebSocket disconnects
            logger.info("Client disconnected from client_to_agent_messaging.")
            break
        except Exception as e:
            logger.error(f"Error in client_to_agent_messaging: {e}")
            # Optionally, try to send an error message to the client
            try:
                await websocket.send_text(json.dumps({"error": str(e), "role": "system"}))
            except: # If sending fails, the connection is likely already gone
                pass
            break # Exit loop on error


def create_app() -> FastAPI:
    app = FastAPI(
        title="Project Serena",
        description="Serena agent powered by Gemini",
        version="0.1.0",
    )

    app.include_router(api_router, prefix="/v1")

    @app.on_event("startup")
    async def startup_event():
        logger.info("FastAPI application starting up...")
        try:
            from app.core.dependencies import get_bigquery_reader

            get_bigquery_reader()
            logger.info("BigQuery client initialized successfully on startup.")
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client on startup: {e}")

    @app.on_event("shutdown")
    async def shutdown_event():
        logger.info("FastAPI application shutting down...")

    return app


app = create_app()
STATIC_DIR = Path("app/static")
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")


@app.get("/")
async def root():
    """Serves the index.html"""
    print("index path:> ",os.path.join(STATIC_DIR, "index.html"))
    return FileResponse(os.path.join(STATIC_DIR, "index.html"))

from app.services.visualization_agent.agent import USER_ID as VIS_USER_ID
from app.services.poster_agent.agent import USER_ID as POSTER_USER_ID

USER_ID_MAP = {
    "visualization_app": VIS_USER_ID,
    "poster_app": POSTER_USER_ID,
}

@app.get("/artifacts/{app_name}/{session_id}/{filename}")
async def get_artifact(app_name: str, session_id: str, filename: str):
    """
    Retrieves an artifact from the in-memory artifact service based on its app, session, and filename.
    """
    logger.info(f"Request for artifact '{filename}' from app '{app_name}' and session '{session_id}'")
    try:
        user_id = USER_ID_MAP.get(app_name)
        if not user_id:
            logger.error(f"No user_id mapping found for app_name: '{app_name}'")
            return Response(status_code=status.HTTP_404_NOT_FOUND, content="Artifact app not found")

        artifact = await _artifact_service.load_artifact(
            app_name=app_name,
            user_id=user_id,
            session_id=session_id,
            filename=filename,
        )

        if artifact and artifact.inline_data:
            image_bytes = artifact.inline_data.data
            mime_type = artifact.inline_data.mime_type
            return Response(content=image_bytes, media_type=mime_type)
        else:
            logger.warning(f"Artifact not found: app='{app_name}', session='{session_id}', file='{filename}'")
            return Response(status_code=status.HTTP_404_NOT_FOUND, content="Artifact not found")

    except Exception as e:
        logger.error(f"Error retrieving artifact: {e}", exc_info=True)
        return Response(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content="Error retrieving artifact")

# @app.get("/artifacts/{session_id}/{filename}")
# async def get_artifact(session_id: str, filename: str):
#     """
#     Retrieves an artifact (like a chart image) from the in-memory artifact service.
#     """
#     logger.info(f"Request for artifact '{filename}' from session '{session_id}'")
#     try:
#         # NOTE: The visualization agent uses its own user_id. We need to use that here.
#         USER_ID_VIZ = "dev_user_01" 
#         artifact = await _artifact_service.load_artifact(
#             app_name="visualization_app",
#             user_id=USER_ID_VIZ,
#             session_id=session_id,
#             filename=filename,
#         )

#         if artifact and artifact.inline_data:
#             image_bytes = artifact.inline_data.data
#             mime_type = artifact.inline_data.mime_type
#             return Response(content=image_bytes, media_type=mime_type)
#         else:
#             logger.warning(f"Artifact not found: session='{session_id}', file='{filename}'")
#             return Response(status_code=status.HTTP_404_NOT_FOUND, content="Artifact not found")

#     except Exception as e:
#         logger.error(f"Error retrieving artifact: {e}", exc_info=True)
#         return Response(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content="Error retrieving artifact")


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(
    websocket: WebSocket,
    session_id: str,
    user_sends_audio_str: str = Query(..., alias="is_audio"), # User input mode: "true" or "false"
    agent_wants_audio_output_str: str = Query("true", alias="agent_wants_audio_output") # Agent output mode
):
    """Client websocket endpoint"""
    await websocket.accept()
    logger.info(
        f"Client #{session_id} connected. User sends audio: '{user_sends_audio_str}'. Agent audio output: '{agent_wants_audio_output_str}'"
    )

    user_sends_audio_bool = user_sends_audio_str.lower() == "true"
    agent_wants_audio_output_bool = agent_wants_audio_output_str.lower() == "true"

    try:
        live_events, live_request_queue = await start_agent_session(
            session_id,
            user_sends_audio=user_sends_audio_bool,
            client_wants_agent_audio_output=agent_wants_audio_output_bool
        )
        logger.info(f"Agent session started for client #{session_id}")

        agent_to_client_task = asyncio.create_task(
            agent_to_client_messaging(websocket, live_events)
        )
        client_to_agent_task = asyncio.create_task(
            client_to_agent_messaging(websocket, live_request_queue)
        )

        done, pending = await asyncio.wait(
            [agent_to_client_task, client_to_agent_task],
            return_when=asyncio.FIRST_COMPLETED,
        )

        for task in pending:
            logger.info(f"Cancelling pending task: {task.get_name()}")
            task.cancel()
        
        if pending:
            await asyncio.gather(*pending, return_exceptions=True)
            logger.info("Pending tasks gathered after cancellation.")

        for task in done:
            if task.exception():
                logger.error(f"Task {task.get_name()} raised an exception: {task.exception()}", exc_info=task.exception())
            else:
                logger.info(f"Task {task.get_name()} completed.")


    except Exception as e:
        logger.error(f"Error in websocket_endpoint for client #{session_id}: {e}", exc_info=True)
        try:
            await websocket.close(code=1011) # Internal error
        except RuntimeError: # If already closed
            pass
    finally:
        logger.info(f"Client #{session_id} disconnected")

# Add WebSocketDisconnect to imports if not already there:
from fastapi import WebSocketDisconnect # Make sure this is imported
File: app\__init__.py

File: app\api\__init__.py

File: app\api\models\bigquery_models.py
# app/api/models/bigquery_models.py
from pydantic import BaseModel, Field, ValidationError
from typing import List, Dict, Any, Optional

class QueryRequest(BaseModel):
    query: str = Field(..., example="SELECT * FROM `bigquery-public-data.thelook_ecommerce.orders` LIMIT 10")

class TableListResponse(BaseModel):
    project: str
    dataset_id: str
    tables: List[str]

class QueryResultRow(BaseModel):
    # This model is flexible for varying BigQuery row structures
    # You might want to define more specific models for specific queries
    data: Dict[str, Any]

class QueryResponse(BaseModel):
    rows: List[QueryResultRow]
    row_count: int
File: app\api\models\__init__.py

File: app\api\v1\routers.py
# app/api/v1/routers.py
from fastapi import APIRouter
from app.api.v1.endpoints import bigquery

api_router = APIRouter()
api_router.include_router(bigquery.router, prefix="/bigquery", tags=["BigQuery"])
File: app\api\v1\__init__.py

File: app\api\v1\endpoints\bigquery.py
# app/api/v1/endpoints/bigquery.py
from fastapi import APIRouter, Depends, HTTPException, status
from app.core.dependencies import get_bigquery_reader
from app.services.bigquery_service import BigQueryReader
from app.api.models.bigquery_models import QueryRequest, TableListResponse, QueryResponse, QueryResultRow

router = APIRouter()

@router.get("/list_tables", response_model=TableListResponse, summary="List tables in a public BigQuery dataset")
async def list_bigquery_tables(
    dataset_project: str = "bigquery-public-data",
    dataset_id: str = "thelook_ecommerce",
    max_results: int = 10,
    bq_reader: BigQueryReader = Depends(get_bigquery_reader)
):
    """
    Lists tables available in a specified BigQuery public dataset.
    """
    tables = bq_reader.list_tables_in_dataset(dataset_project, dataset_id, max_results)
    if tables is None: # Indicates an error occurred in the service layer
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve tables from BigQuery. Check logs for details."
        )
    elif not tables: # Indicates dataset not found or empty
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset '{dataset_project}.{dataset_id}' not found or contains no tables."
        )
    return TableListResponse(project=dataset_project, dataset_id=dataset_id, tables=tables)

@router.post("/query", response_model=QueryResponse, summary="Execute a custom SQL query on BigQuery")
async def execute_bigquery_query(
    request: QueryRequest,
    bq_reader: BigQueryReader = Depends(get_bigquery_reader)
):
    """
    Executes a SQL query on BigQuery.
    """
    rows = bq_reader.execute_query(request.query)
    if rows is None: # Indicates an error occurred
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to execute BigQuery query. Check logs for details."
        )
    
    # Convert list of dicts to list of QueryResultRow models
    formatted_rows = [QueryResultRow(data=row) for row in rows]

    return QueryResponse(rows=formatted_rows, row_count=len(formatted_rows))
File: app\api\v1\endpoints\__init__.py

File: app\core\config.py
# app/core/config.py
from pydantic_settings import BaseSettings, SettingsConfigDict
import os

class Settings(BaseSettings):
    GOOGLE_CLOUD_PROJECT_ID: str = "hackathon-agents" # Default, but better to set via env var
    BIGQUERY_SERVICE_ACCOUNT_KEY_PATH: str = "hackathon-agents-044c975e8972.json"
    model_config = SettingsConfigDict(env_file=".env", extra='ignore')

settings = Settings()

# Optional: Add a check if the service account path is relative and make it absolute
if not os.path.isabs(settings.BIGQUERY_SERVICE_ACCOUNT_KEY_PATH):
    settings.BIGQUERY_SERVICE_ACCOUNT_KEY_PATH = os.path.abspath(settings.BIGQUERY_SERVICE_ACCOUNT_KEY_PATH)
File: app\core\dependencies.py
# app/core/dependencies.py
from app.services.bigquery_service import BigQueryReader
from app.core.config import settings
import functools
import logging

logger = logging.getLogger(__name__)

# Use functools.lru_cache to ensure the BigQueryReader is initialized only once
@functools.lru_cache()
def get_bigquery_reader() -> BigQueryReader:
    """
    Dependency that provides a BigQueryReader instance.
    This instance is created once and reused across all requests.
    """
    try:
        bq_reader = BigQueryReader(
            project_id=settings.GOOGLE_CLOUD_PROJECT_ID,
            service_account_key_path=settings.BIGQUERY_SERVICE_ACCOUNT_KEY_PATH
        )
        logger.info("BigQueryReader dependency successfully initialized.")
        return bq_reader
    except (FileNotFoundError, ConnectionError) as e:
        logger.critical(f"Failed to initialize BigQueryReader: {e}")
        # In a real application, you might want to raise a custom exception
        # that FastAPI can catch and return a 500 status code.
        raise # Re-raise to prevent the application from starting if critical dependency fails
    except Exception as e:
        logger.critical(f"An unexpected error occurred during BigQueryReader initialization: {e}")
        raise
File: app\core\__init__.py

File: app\services\bigquery_service.py
# app/services/bigquery_service.py
import os
from google.cloud import bigquery
from google.cloud.exceptions import NotFound, GoogleCloudError
import logging
import traceback

# Configure logging for better visibility within the service
# Note: FastAPI will handle global logging usually, but this is good for internal service logs.
logging.basicConfig(
    level=logging.INFO,  # Changed to INFO for better visibility of service operations
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class BigQueryReader:
    """
    A class to encapsulate BigQuery read operations using a service account.
    Focuses on querying public datasets.
    """

    def __init__(self, project_id: str, service_account_key_path: str):
        """
        Initializes the BigQuery client.

        Args:
            project_id (str): Your Google Cloud Project ID. This is required for billing
                              and job execution context, even for public datasets.
            service_account_key_path (str): Path to your service account JSON key file.
        """
        if not os.path.exists(service_account_key_path):
            logger.error(
                f"Service account key file not found at: {service_account_key_path}"
            )
            raise FileNotFoundError(
                f"Service account key file not found at: {service_account_key_path}"
            )

        self.project_id = project_id
        self.service_account_key_path = service_account_key_path
        self.client = None
        self._initialize_client()
        logger.info(f"BigQueryReader initialized for project: {self.project_id}")

    def _initialize_client(self):
        """
        Internal method to set up the BigQuery client.
        Sets GOOGLE_APPLICATION_CREDENTIALS environment variable.
        """
        try:
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self.service_account_key_path
            self.client = bigquery.Client(project=self.project_id)
            # Test connection by making a small request
            # self.client.list_projects(max_results=1) # A simple test if needed
            logger.info(
                f"BigQuery client successfully initialized for project: {self.client.project}"
            )
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client: {e}")
            raise ConnectionError(
                f"Could not connect to BigQuery. Check credentials and project ID. Error: {e}"
            )

    def list_tables_in_dataset(
        self, project: str = 'bigquery-public-data', dataset_id: str= 'thelook_ecommerce', max_results: int = 10
    ) -> list:
        """
        Lists tables in a specified BigQuery dataset.
        Default project is bigquery-public-data and the default dataset is thelook_ecommerce.

        Args:
            project (str): The project ID where the dataset resides (e.g., 'bigquery-public-data').
            dataset_id (str): The ID of the dataset (e.g., 'thelook_ecommerce').
            max_results (int): Maximum number of tables to retrieve.

        Returns:
            list: A list of table IDs (strings). Returns an empty list on error.
        """
        logger.info(f"Attempting to list tables in '{project}.{dataset_id}'...")
        try:
            dataset_ref = bigquery.DatasetReference(project, dataset_id)
            tables = []
            for table_item in self.client.list_tables(
                dataset_ref, max_results=max_results
            ):
                tables.append(table_item.table_id)
            logger.info(
                f"Successfully listed {len(tables)} tables in '{project}.{dataset_id}'."
            )
            return tables
        except NotFound:
            logger.warning(
                f"Dataset '{project}.{dataset_id}' not found or inaccessible."
            )
            return []
        except GoogleCloudError as e:
            logger.error(
                f"Google Cloud Error listing tables in '{project}.{dataset_id}': {e}"
            )
            return []
        except Exception as e:
            logger.error(f"An unexpected error occurred while listing tables: {e}")
            return []

    def execute_query(self, query: str) -> tuple:
        """
        Executes a SQL query on BigQuery and returns the results.
        Default project is `bigquery-public-data` and the default dataset is `thelook_ecommerce`.

        Args:
            query (str): The SQL query string to execute.

        Returns:
            tuple: The SQL query and a list of BigQuery Row objects, or an empty list if an error occurs.
        """
        logger.info("Executing BigQuery query...")
        try:
            print("Generated sql:> ",query)
            query_job = self.client.query(query)  # API request
            results = query_job.result()  # Waits for job to complete
            rows = [
                dict(row) for row in results
            ]  # Convert rows to dictionaries for easier handling
            logger.info(f"Query executed successfully. Fetched {len(rows)} rows.")
            return (rows)
        except Exception as e:
            error_message = f"An unexpected error occurred during query execution: {e}"
            logger.error(error_message)
            logger.error(traceback.format_exc())
        return (traceback.format_exc())

File: app\services\__init__.py

File: app\services\agents\agent.py
from google.adk.agents import Agent, LoopAgent, SequentialAgent
from app.core.dependencies import get_bigquery_reader
from app.services.agents.agents_tools import get_similar_tables_tool, exit_loop, say_hello, say_goodbye

bq_rdr= get_bigquery_reader()

# print('---->>>>',get_similar_tables_tool('How many users are there in the dataset?'))# Example usage of the tool

MODEL_GEMINI_2_0_FLASH="gemini-2.0-flash-live-001" #"gemini-2.0-flash-exp" # Example model name, replace with actual model name
MODEL_GEMINI_2_0_FLASH_LITE= "gemini-2.0-flash-lite"


greeting_agent = Agent(
    model=MODEL_GEMINI_2_0_FLASH,
    name="greeting_agent",
    instruction="You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.",
    description="Handles simple greetings and hellos using the 'say_hello' tool.",
    tools=[say_hello],
)

farewell_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="farewell_agent",
        instruction="You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.",
        description="Handles simple farewells and goodbyes using the 'say_goodbye' tool.",
        tools=[say_goodbye],
    )

sql_generator_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_generator_agent",
        description="Handles the generation of SQL queries based on user requests.",
        instruction="""You are the Bigquery compatible SQL agent. Your task is to generate SQL queries based on user requests. Use the 'get_similar_tables_tool' tool to get access to the metadata of the database and generate SQL queries accordingly. 
        Do not perform any other actions.
        The SQL queries you generate should be compatible with BigQuery syntax.
        
        ## SQL Query Format
        - Use BigQuery compatible SQL syntax.
        - Example:
            - **User Query**: "How many users are there in the dataset?"
            - **SQL Query**: `SELECT count(*) FROM bigquery-public-data.thelook_ecommerce.users`
            - Ensure the SQL query is well-formed and executable in BigQuery.
        If the user asks for similar tables, use the 'get_similar_tables_tool' to retrieve relevant tables and their metadata.
        """,
        tools=[get_similar_tables_tool],
        output_key="sql_query",
    )

sql_executor_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_executor_agent",
        description="Handles the execution of SQL queries and returns results.",
        instruction="""You are the Bigquery compatible SQL executor agent. Your task is to execute the generated SQL queries. 
        #GENERATED SQL QUERY:> {{sql_query}}
        Use the 'bq_rdr.execute_query' tool to execute the SQL queries on Bigquery and get the result.
        """,
        tools=[bq_rdr.execute_query],
        output_key="sql_execution_result",
    )

sql_refiner_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_refiner_agent",
        description="Handles the refinement/correction of SQL queries and returns the refined query.",
        instruction="""You are the Bigquery compatible SQL refiner agent. Your task is to refine the SQL queries generated by the 'sql_generator_agent'.
        You will be provided with the original SQL query and the SQL execution result.
        #ORIGINAL SQL QUERY:> {{original_sql_query}}
        If the SQL execution result is empty or indicates an error, refine the SQL query to correct it.
        If the SQL execution result is valid, return the original SQL query as it is and call the 'exit_loop' tool to end the iterative process.
        """,
        tools=[bq_rdr.execute_query,exit_loop],
        output_key="sql_query",
    )

refinement_loop = LoopAgent(
    name="RefinementLoop",
    sub_agents=[
        sql_executor_agent,
        sql_refiner_agent,
    ],
    max_iterations=5 # Limit loops
)

root_sql_agent = SequentialAgent(
    name="SequentialLoopPipeline",
    sub_agents=[
        sql_generator_agent, # Run first to create initial doc
        refinement_loop       # Then run the critique/refine loop
    ],
    description="Generate the SQL query based on user request, execute it, and refine it iteratively until the query is correct or no further refinements are needed",
)



general_agent = Agent(
    # A unique name for the agent.
    name="serena",
    model=MODEL_GEMINI_2_0_FLASH,
    description="General conversation agent",
    instruction=f"""
    # Jarvis: The Helpful Assistant
        You are Jarvis, a helpful assistant capable of healthy conversation and resolving queries related to databases using the appropriate tools provided. Follow these guidelines:

        ## Specialized Sub-Agents
        - **greeting_agent**: Handles simple greetings like "Hi" or "Hello". Delegate such queries to this agent.
        - **farewell_agent**: Handles simple farewells like "Bye" or "See you". Delegate such queries to this agent.
        - **root_sql_agent**: Handles database-related queries, generating SQL queries, executing them, and refining them iteratively.

        ## Query Analysis
        - If the user's query is a greeting, delegate to **greeting_agent**.
        - If the user's query is a farewell, delegate to **farewell_agent**.
        - For database-related queries, delegate to **root_sql_agent**.
        - If the query does not fit any of the above categories, respond with a friendly message indicating that you can assist with database-related queries or greetings/farewells.
        - For list table database-related queries, invoke 'bq_rdr.list_tables_in_dataset' tool to get the list.

        ## Conversation Closure
        - Aim to end every interaction on a positive note.
    """,
    tools=[bq_rdr.list_tables_in_dataset],
    sub_agents=[greeting_agent, farewell_agent, root_sql_agent]
)
File: app\services\agents\agents.py
from google.adk.agents import Agent, LoopAgent, SequentialAgent
from app.core.dependencies import get_bigquery_reader
from app.services.agents.agents_tools import get_similar_tables_tool, exit_loop, say_hello, say_goodbye

bq_rdr= get_bigquery_reader()
MODEL_GEMINI_2_0_FLASH="gemini-2.0-flash-live-001"

sql_generator_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_generator_agent",
        description="Handles the generation of SQL queries based on user requests.",
        instruction="""You are the Bigquery compatible SQL agent. Your task is to generate SQL queries based on user requests. Use the 'get_similar_tables_tool' tool to get access to the metadata of the database and generate SQL queries accordingly. 
        Do not perform any other actions.
        The SQL queries you generate should be compatible with BigQuery syntax.
        
        ## SQL Query Format
        - Use BigQuery compatible SQL syntax.
        - Example:
            - **User Query**: "How many users are there in the dataset?"
            - **SQL Query**: `SELECT count(*) FROM bigquery-public-data.thelook_ecommerce.users`
            - Ensure the SQL query is well-formed and executable in BigQuery.
        If the user asks for similar tables, use the 'get_similar_tables_tool' to retrieve relevant tables and their metadata.
        """,
        tools=[get_similar_tables_tool],
        output_key="sql_query",)

sql_executor_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_executor_agent",
        description="Handles the execution of SQL queries and returns results.",
        instruction="""You are the Bigquery compatible SQL executor agent. Your task is to execute the generated SQL queries. 
        #GENERATED SQL QUERY:> {{sql_query}}
        Use the 'bq_rdr.execute_query' tool to execute the SQL queries on Bigquery and get the result.
        """,
        tools=[bq_rdr.execute_query],
        output_key="sql_execution_result",
        include_contents='none'
    )

sql_refiner_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_refiner_agent",
        description="Handles the refinement/correction of SQL queries and returns the refined query.",
        instruction="""You are the Bigquery compatible SQL refiner agent. Your task is to refine the SQL queries generated by the 'sql_generator_agent'.
        You will be provided with the original SQL query and the SQL execution result.
        #ORIGINAL SQL QUERY:> {{original_sql_query}}
        If the SQL execution result is empty or indicates an error, refine the SQL query to correct it.
        If the SQL execution result is valid, return the original SQL query as it is and call the 'exit_loop' tool to end the iterative process.
        
        #SQL EXECUTION RESULT:> {{sql_execution_result}}
        """,
        tools=[exit_loop],
        output_key="sql_query",
    )

from pydantic import BaseModel, Field

class SQLOutput(BaseModel):
    final_sql: str = Field(description="The final SQL query after refinement.")
    sql_results: list = Field(default_factory=list, description="The results of the SQL query execution.")

sql_extractor_agent = Agent(
        model=MODEL_GEMINI_2_0_FLASH,
        name="sql_extractor_agent",
        description="Handles the extraction of the SQL query from the SQL execution result.",
        instruction="""You are the Bigquery SQL extractor. Your task is to extract the SQL query from the SQL execution result.
        You will be provided with the SQL execution result.
        
        #Following are the SQL and its results:> 
        {{sql_query}}
        {{sql_execution_result}}
        """,
        output_schema=SQLOutput,
        output_key="output",
        include_contents='none'
    )

refinement_loop = LoopAgent(
    name="RefinementLoop",
    sub_agents=[
        sql_executor_agent,
        sql_refiner_agent,
    ],
    max_iterations=5 # Limit loops
)

root_sql_agent = SequentialAgent(
    name="SequentialLoopPipeline",
    sub_agents=[
        sql_generator_agent, # Run first to create initial doc
        refinement_loop,# Then run the critique/refine loop
        sql_extractor_agent
    ],
    description="Generate the SQL query based on user request, execute it, and refine it iteratively until the query is correct or no further refinements are needed",
)

general_agent = Agent(
    # A unique name for the agent.
    name="serena",
    model=MODEL_GEMINI_2_0_FLASH,
    description="General conversation agent",
    instruction=f"""
    # Jarvis: The Helpful Assistant
        You are Jarvis, a helpful assistant capable of healthy conversation and resolving queries related to databases using the appropriate tools provided. Follow these guidelines:
        - **root_sql_agent**: Handles database-related queries, generating SQL queries, executing them, and refining them iteratively.

        ## Query Analysis
        - For database-related queries, delegate to **root_sql_agent**.
        - If the query does not fit any of the above categories, respond with a friendly message indicating that you can assist with database-related queries or greetings/farewells.
        - For list table database-related queries, invoke 'bq_rdr.list_tables_in_dataset' tool to get the list.

        ## Conversation Closure
        - Aim to end every interaction on a positive note.
    """,
    tools=[bq_rdr.list_tables_in_dataset],
    sub_agents=[root_sql_agent]
)















#--------------------------------------new agent ASHISH
import datetime
from zoneinfo import ZoneInfo
def get_weather(city: str) -> dict:
    """Retrieves the current weather report for a specified city.

    Args:
        city (str): The name of the city for which to retrieve the weather report.

    Returns:
        dict: status and result or error msg.
    """
    if city.lower() == "new york":
        return {
            "status": "success",
            "report": (
                "The weather in New York is sunny with a temperature of 25 degrees"
                " Celsius (77 degrees Fahrenheit)."
            ),
        }
    else:
        return {
            "status": "error",
            "error_message": f"Weather information for '{city}' is not available.",
        }


def get_current_time(city: str) -> dict:
    """Returns the current time in a specified city.

    Args:
        city (str): The name of the city for which to retrieve the current time.

    Returns:
        dict: status and result or error msg.
    """

    if city.lower() == "new york":
        tz_identifier = "America/New_York"
    else:
        return {
            "status": "error",
            "error_message": (
                f"Sorry, I don't have timezone information for {city}."
            ),
        }

    tz = ZoneInfo(tz_identifier)
    now = datetime.datetime.now(tz)
    report = (
        f'The current time in {city} is {now.strftime("%Y-%m-%d %H:%M:%S %Z%z")}'
    )
    return {"status": "success", "report": report}


root_weather_agent = Agent(
    name="weather_time_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    description=(
        "Agent to answer questions about the time and weather in a city."
    ),
    instruction=(
        "You are a helpful agent who can answer user questions about the time and weather in a city."
    ),
    tools=[get_weather, get_current_time],)

general_greeting_agent = Agent(
    name="general_greeting_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    description=(
        "Agent to answer questions relating to user general query"
    ),
    instruction=(
        """You are a helpful agent who can answer user questions and have a great open conversation.
        You can speak in English, Hindi, or any other language."""
    ),)
File: app\services\agents\agents_tools.py
# to be defined later
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from google.adk.tools.tool_context import ToolContext
from typing import Optional

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-exp-03-07")
vector_store = Chroma(
    collection_name="StyleHubMetadataDB",  # Name of the collection in Chroma
    embedding_function=embeddings,
    persist_directory="./chroma_db",  # Where to save data locally, remove if not necessary
)


def say_hello(name: Optional[str] = None) -> str:
    """Provides a simple greeting. If a name is provided, it will be used.

    Args:
        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.

    Returns:
        str: A friendly greeting message.
    """
    if name:
        greeting = f"Hello, {name}!"
        print(f"--- Tool: say_hello called with name: {name} ---")
    else:
        greeting = "Hello there!" # Default greeting if name is None or not explicitly passed
        print(f"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---")
    return greeting

def say_goodbye() -> str:
    """Provides a simple farewell message to conclude the conversation."""
    print(f"--- Tool: say_goodbye called ---")
    return "Goodbye! Have a great day."


def exit_loop(tool_context: ToolContext):
    """Call this function ONLY when the critique indicates no further changes are needed, signaling the iterative process should end."""
    print(f"  [Tool Call] exit_loop triggered by {tool_context.agent_name}")
    tool_context.actions.escalate = True
    # Return empty dict as tools should typically return JSON-serializable output
    return {}

def get_similar_tables_tool(user_query:str):
    """
    Function to get similar tables based on user query.
    Uses vector store to find relevant tables.
    
    Args:
        user_query (str): The user's query to search for similar tables.
        
    Returns:
        list: A list of tuples containing the table name and similarity score.
    """
    results = vector_store.similarity_search_with_score(user_query, k=5)
    return [(res.page_content, score) for res, score in results]
File: app\services\agents\__init__.py

File: app\services\bq_agent\agent.py
import asyncio
import uuid
import os
import json
from typing import Dict, Any

from google.adk.agents import LlmAgent, SequentialAgent
from google.adk.agents.callback_context import CallbackContext
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.genai.types import Content, Part
from google.cloud import bigquery
from google.cloud.exceptions import NotFound, GoogleCloudError
from dotenv import load_dotenv
import logging
import traceback

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Constants ---
APP_NAME = "sql_pipeline_app"
USER_ID = "dev_user_01"
MODEL_GEMINI_2_0_FLASH = "gemini-2.0-flash-001"

# ==============================================================================
# YOUR HELPER FUNCTIONS AND CLASSES (Unchanged)
# ==============================================================================

def json_to_paragraphs(file_path):
    # TODO: Consider loading this data once at startup instead of on every run.
    with open(file_path, 'r') as file:
        data = json.load(file)
    paragraphs = []
    for table in data.get('tables', []):
        table_name = table.get('table_name', 'Unnamed Table')
        table_description = table.get('table_description', 'No description available.')
        paragraph = f"Table '{table_name}': {table_description}\n"
        paragraph += "Columns:\n"
        for column in table.get('columns', []):
            column_name = column.get('column_name', 'Unnamed Column')
            column_type = column.get('column_type', 'Unknown Type')
            column_description = column.get('column_description', 'No description available.')
            is_primary_key = column.get('is_primary_key', False)
            primary_key_info = " (Primary Key)" if is_primary_key else ""
            foreign_key_info = ""
            if 'foreign_key' in column:
                fk_table = column['foreign_key'].get('reference_table', 'Unknown Table')
                fk_column = column['foreign_key'].get('reference_column', 'Unknown Column')
                foreign_key_info = f" (Foreign Key references {fk_table}.{fk_column})"
            paragraph += f"  - {column_name} ({column_type}): {column_description}{primary_key_info}{foreign_key_info}\n"
        paragraphs.append(paragraph)
    return "\n".join(paragraphs)

def bigquery_metdata_extraction_tool():
    """ Extracts BigQuery table metadata from a JSON file."""
    # TODO: Replace hardcoded path with environment variable or configuration.
    # json_path = r"D:\3_hackathon\1_llm_agent_hackathon_google\cautious-palm-tree\dataset_info.json"
    current_directory = os.path.dirname(os.path.abspath(__file__))
    root_directory = os.path.abspath(os.path.join(current_directory, '..', '..','..'))
    json_path = os.path.join(root_directory, 'dataset_info.json')

    if not os.path.exists(json_path):
        raise FileNotFoundError(f"Metadata JSON file not found at: {json_path}")
    return json_to_paragraphs(json_path)

class BigQueryReader:
    """A class to encapsulate BigQuery read operations."""
    def __init__(self, project_id: str, service_account_key_path: str):
        if not os.path.exists(service_account_key_path):
            logger.error(f"Service account key file not found at: {service_account_key_path}")
            raise FileNotFoundError(f"Service account key not found at: {service_account_key_path}")
        self.project_id = project_id
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = service_account_key_path
        try:
            self.client = bigquery.Client(project=self.project_id)
            logger.info(f"BigQuery client successfully initialized for project: {self.client.project}")
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client: {e}")
            raise ConnectionError(f"Could not connect to BigQuery. Check credentials. Error: {e}")

    def execute_query(self, query: str) -> Any:
        """Executes a SQL query and returns results or an error string."""
        logger.info(f"Executing BigQuery query: {query[:100]}...")
        try:
            query_job = self.client.query(query)
            results = query_job.result()
            rows = [dict(row) for row in results]
            logger.info(f"Query executed successfully. Fetched {len(rows)} rows.")
            return rows
        except Exception:
            error_message = f"Error during query execution: {traceback.format_exc()}"
            logger.error(error_message)
            return {"error": error_message}

# Initialize the BigQuery reader tool.
# TODO: Replace hardcoded paths with environment variables for better security and portability.
SERVICE_ACCOUNT_PATH = r"..\..\hackathon-agents-044c975e8972.json"

current_directory = os.path.dirname(os.path.abspath(__file__))
root_directory = os.path.abspath(os.path.join(current_directory, '..', '..','..'))
SERVICE_ACCOUNT_PATH = os.path.join(root_directory, 'hackathon-agents-044c975e8972.json')

bq_reader = BigQueryReader(project_id="hackathon-agents", service_account_key_path=SERVICE_ACCOUNT_PATH)

# ==============================================================================
# YOUR AGENT DEFINITIONS (Unchanged, but with added context)
# ==============================================================================

def initialize_state_var(callback_context: CallbackContext):
    """Callback to initialize the session state before the pipeline runs."""
    callback_context.state["PROJECT"] = "hackathon-agents"
    callback_context.state["BQ_LOCATION"] = "us-central1"
    callback_context.state["DATASET"] = "StyleHub"
    # This pre-loads the metadata so the agents can use it.
    callback_context.state["bigquery_metadata"] = bigquery_metdata_extraction_tool()
    logger.info("Session state initialized with BigQuery project, location, and metadata.")

# Agent 1: Understands the user's query
query_understanding_agent = LlmAgent(
    name="query_understanding_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    instruction="""
    You are a data analyst. Your role is to understand the user's natural language query.
    Identify the BigQuery tables and columns needed to answer the query.
    If the query is ambiguous, ask clarifying questions.
    Use the provided BigQuery metadata: {bigquery_metadata}
    Format the output as a JSON object with table.column as keys and your reasoning as values.
    """,
    output_key="query_understanding_output"
)

# Agent 2: Generates the initial SQL query
query_generation_agent = LlmAgent(
    name="query_generation_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    instruction="""
    You are a BigQuery SQL writer. Your job is to write standard BigQuery SQL.
    - Use the analysis from the previous agent: {query_understanding_output}
    - Use project '{PROJECT}', location '{BQ_LOCATION}', and dataset '{DATASET}'.
    - Use the following metadata: <METADATA>{bigquery_metadata}</METADATA>
    Output only the generated query as a raw text string.
    """,
    output_key="query_generation_output"
)

# Agent 3: Reviews and refactors the SQL
query_review_rewrite_agent = LlmAgent(
    name="query_review_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    instruction="""
    You are a BigQuery SQL reviewer and rewriter.
    - Original analysis: {query_understanding_output}
    - Initial query: {query_generation_output}
    - Use project '{PROJECT}', location '{BQ_LOCATION}', dataset '{DATASET}'.
    - Use metadata: {bigquery_metadata}
    Review and rewrite the query based on these rules:
    - Ensure all columns have proper aliases.
    - Add 'LIMIT 10' to SELECT queries that might fetch many records.
    - Ensure filter conditions are case-insensitive (e.g., use LOWER() or UPPER()).
    - Convert datetime/timestamp columns to strings for display.
    Output only the final, rewritten query as a raw text string.
    """,
    output_key="query_review_rewrite_output"
)

# Agent 4: Executes the query
# This agent's primary job is to format the input for the tool call.
query_execution_agent = LlmAgent(
    name="query_execution_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    instruction="""
    You are a BigQuery SQL executor.
    You must execute the provided SQL query using the `execute_query` tool.
    The query to execute is: {query_review_rewrite_output}
    """,
    tools=[bq_reader.execute_query],
    output_key="query_execution_output"
)

# The complete sequential pipeline
sql_pipeline_agent = SequentialAgent(
    name="SQLPipelineAgent",
    sub_agents=[
        query_understanding_agent,
        query_generation_agent,
        query_review_rewrite_agent,
        query_execution_agent,
    ],
    before_agent_callback=initialize_state_var,
)

# Session Service Setup
_session_service = InMemorySessionService()

# ==============================================================================
# WRAPPER FUNCTION TO EXECUTE THE PIPELINE
# ==============================================================================
async def execute_sql_pipeline(user_query: str) -> Dict[str, Any]:
    """
    Executes the complete SQL generation and execution pipeline asynchronously.

    Args:
        user_query (str): A natural language query about the data.
                          Example: "Show me the top 10 products by sales."

    Returns:
        dict: A dictionary containing the results from each stage of the pipeline.
    """
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        current_session_id = str(uuid.uuid4())
        print(f"▶️  Running SQL pipeline for query: '{user_query[:50]}...'")

        await _session_service.create_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        runner = Runner(
            agent=sql_pipeline_agent,
            app_name=APP_NAME,
            session_service=_session_service,
        )

        initial_message = Content(role="user", parts=[Part(text=user_query)])

        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass  # Wait for the runner to complete

        session_state_data = await _session_service.get_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # Extract the output from each step using the defined output_keys
        understanding_output = session_state_data.state.get("query_understanding_output")
        generated_sql = session_state_data.state.get("query_generation_output")
        reviewed_sql = session_state_data.state.get("query_review_rewrite_output")
        execution_result = session_state_data.state.get("query_execution_output")

        print("✅ SQL pipeline completed successfully.")
        return {
            "user_query": user_query,
            "understanding": understanding_output or "Not generated.",
            "generated_sql": generated_sql or "Not generated.",
            "reviewed_sql": reviewed_sql or "Not generated.",
            "execution_result": execution_result or "Not executed.",
        }

    except Exception as e:
        print(f"❌ Pipeline failed with an error: {e}")
        traceback.print_exc()
        return {"error": str(e)}

# --- Example Usage ---
async def main():
    """Main function to demonstrate running the SQL pipeline."""
    print("--- Running SQL Pipeline Agent ---")
    
    # IMPORTANT: Ensure you have:
    # 1. A .env file with your GOOGLE_API_KEY.
    # 2. The BigQuery Service Account JSON key at the specified path.
    # 3. The dataset_info.json file at the specified path.

    # user_input = "how many users are there?"
    user_input = "what are the products with cost price more than 100?"
    
    result = await execute_sql_pipeline(user_input)

    print("\n--- PIPELINE RESULTS ---")
    if result.get("error"):
        print(f"Error: {result['error']}")
    else:
        # Pretty print the results
        for key, value in result.items():
            print(f"\n--- {key.replace('_', ' ').upper()} ---")
            if isinstance(value, (dict, list)):
                print(json.dumps(value, indent=2))
            else:
                print(value)
    print("------------------------")


if __name__ == "__main__":
    asyncio.run(main())
File: app\services\bq_agent\__init__.py
from . import agent
File: app\services\greeting_agent\agent.py
import asyncio
import uuid
import os
from typing import Dict, Any

from google.adk.agents import LlmAgent
from google.genai.types import Content, Part
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from dotenv import load_dotenv

# --- Constants ---
# It's good practice to define these for clarity and reusability.
APP_NAME = "greeting_app"
USER_ID = "dev_user_01"
GEMINI_MODEL = "gemini-2.0-flash-001"

# --- Agent and Session Service Setup (Instantiated once on import) ---

general_greeting_agent = LlmAgent(
    name="general_greeting_agent",
    model=GEMINI_MODEL,
    instruction=(
        """You are a friendly and helpful agent who engages in meaningful conversations with users, answering their questions and making them feel valued.
        You can speak in English, Hindi, or any other language as per the user's preference."""
    ),
    description=(
        "An engaging agent to answer user queries and foster meaningful connections."
    ),
    output_key="greeting_response",
)

_session_service = InMemorySessionService()


async def execute_greeting(user_query: str) -> Dict[str, Any]:
    """
    Executes the greeting agent to get a conversational response.

    This function sets up a session, runs the LlmAgent with the user's query,
    and retrieves the generated response. It mirrors the structure of your
    working reference code.

    Args:
        user_query (str): The user's message or question.
                          Example: "Hello, how are you today?"

    Returns:
        dict: A dictionary containing the agent's response or an error message.
            - 'response' (str): The conversational reply from the agent.
            - 'error' (str, optional): An error message if the execution fails.
    """
    # Load environment variables (e.g., GOOGLE_API_KEY) from a .env file
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        # Step 1: Create a unique session for this interaction.
        # This keeps conversations isolated.
        current_session_id = str(uuid.uuid4())
        print(f"▶️  Running Greeting Agent for query: '{user_query[:50]}...'")
        await _session_service.create_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # Step 2: Instantiate the Runner.
        # The runner orchestrates the execution of the agent within the session.
        runner = Runner(
            agent=general_greeting_agent,
            app_name=APP_NAME,
            session_service=_session_service,
        )

        # Step 3: Format the input for the agent.
        # The ADK standard is to use Content/Part objects.
        initial_message = Content(role="user", parts=[Part(text=user_query)])

        # Step 4: Execute the agent asynchronously.
        # We loop through the runner's async generator to let it run to completion.
        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass  # The loop consumes the async generator until the agent is done.

        # Step 5: Retrieve the final state of the session.
        session_state_data = await _session_service.get_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # Step 6: Extract the output from the session state.
        # We use the 'output_key' ("greeting_response") we defined in the LlmAgent.
        final_response = session_state_data.state.get("greeting_response")

        print("✅ Greeting Agent completed successfully.")
        return {
            "response": (
                final_response if final_response else "No response was generated."
            )
        }

    except Exception as e:
        import traceback

        print(f"❌ Greeting Agent failed with an error: {e}")
        traceback.print_exc()
        return {"error": str(e), "response": "Agent execution failed."}


# --- Example Usage ---
async def main():
    """Main function to demonstrate running the greeting agent."""
    print("--- Running Greeting Agent ---")

    user_input = "Hello there! How are you doing today?"
    result = await execute_greeting(user_input)

    if result.get("error"):
        print(f"\nError: {result['error']}")
    else:
        print(f"\nUser Query: {user_input}")
        print(f"Agent Response: {result['response']}")


if __name__ == "__main__":
    # This allows you to run the script directly to test the function.
    asyncio.run(main())

File: app\services\greeting_agent\prompt.py

File: app\services\greeting_agent\__init__.py
from . import agent
File: app\services\poster_agent\agent.py
import asyncio
import uuid
import os
import json
from typing import Dict, Any
import logging
import traceback

from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.adk.artifacts import InMemoryArtifactService
from google.adk.runners import Runner
from google.adk.tools import load_artifacts
from google.adk.tools import ToolContext
from google.genai import Client
from google.genai.types import Content, Part
from google.genai import types
from dotenv import load_dotenv
from app.services.visualization_agent.agent import _artifact_service
# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Constants ---
APP_NAME = "poster_app"
USER_ID = "dev_user_01"
MODEL_GEMINI_2_0_FLASH = "gemini-2.0-flash-001"

# ==============================================================================
# POSTER AGENT AND TOOL DEFINITIONS (Your provided code)
# ==============================================================================

# Initialize the GenAI client once
# Ensure you have your GOOGLE_API_KEY in a .env file or set as an environment variable
load_dotenv()
client = Client()

async def generate_image(prompt: str, tool_context: ToolContext):
  """Generates an image based on the prompt."""
  try:
    logger.info(f"Generating image with prompt: '{prompt[:70]}...'")
    response = client.models.generate_images(
        model='imagen-3.0-generate-002', # Using a powerful image model
        prompt=prompt,
        config={'number_of_images': 1},
    )
    if not response.generated_images:
      logger.error("Image generation failed, no images returned.")
      return {'status': 'failed', 'detail': 'The model did not return any images.'}
      
    image_bytes = response.generated_images[0].image.image_bytes
    
    filename = 'generated_image.png'
    await tool_context.save_artifact(
        filename,
        types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
    )
    logger.info(f"Successfully saved image as artifact: '{filename}'")
    return {
        'status': 'success',
        'detail': f'Image generated successfully and stored in artifact: {filename}',
        'filename': filename,
    }
  except Exception as e:
      error_message = f"Error generating image: {e}\n{traceback.format_exc()}"
      logger.error(error_message)
      return {"status": "error", "detail": error_message}

image_generator_agent = LlmAgent(
    model=MODEL_GEMINI_2_0_FLASH,
    name='image_generator_agent',
    description="An agent that generates images based on user prompts.",
    instruction="""You are an agent whose job is to generate an image based on the user's prompt.
When the user asks you to create an image, you MUST call the `generate_image` tool with a detailed, descriptive prompt to create the best possible image.
""",
    tools=[generate_image, load_artifacts],
    output_key="image_generation_summary" # Added for clarity in session state
)


# ==============================================================================
# AGENT EXECUTION WRAPPER
# ==============================================================================

# Instantiate services once on import
_session_service = InMemorySessionService()
# _artifact_service = InMemoryArtifactService()

async def execute_poster_pipeline(user_prompt: str,tool_context: ToolContext) -> Dict[str, Any]:
    """
    Executes the poster agent pipeline to generate an image artifact.

    Args:
        user_prompt (str): The user's natural language request for an image.

    Returns:
        dict: A dictionary containing the session ID and information about the generated artifact.
    """
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        current_session_id = str(uuid.uuid4())
        print(f"▶️  Running Poster pipeline for prompt: '{user_prompt[:50]}...'")

        await _session_service.create_session(
            app_name=APP_NAME,
            user_id=USER_ID,
            session_id=current_session_id,
            state={}, # Initial state can be empty
        )

        # The Runner is initialized with the services.
        runner = Runner(
            agent=image_generator_agent,
            app_name=APP_NAME,
            session_service=_session_service,
            artifact_service=_artifact_service,
        )

        initial_message = Content(role="user", parts=[Part(text=user_prompt)])

        # Run the agent until it completes its task
        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass

        # Verify the artifact was saved by trying to load it
        generated_filename = "generated_image.png"
        final_artifact = await _artifact_service.load_artifact(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id, filename=generated_filename
        )

        print("✅ Poster pipeline completed successfully.")
        return {
            "session_id": current_session_id,
            "app_name": APP_NAME,
            "artifact_saved": generated_filename if final_artifact else "No",
            "artifact_size_bytes": len(final_artifact.inline_data.data) if final_artifact else 0,
        }

    except Exception as e:
        print(f"❌ Pipeline failed with an error: {e}")
        traceback.print_exc()
        return {"error": str(e)}

# --- Example Usage ---
async def main():
    """Main function to demonstrate running the poster agent pipeline."""
    print("--- Running Poster Generation Agent ---")

    # Example prompt for the poster agent
    user_input = "Create a photorealistic image of a futuristic city skyline at sunset, with flying cars and holographic advertisements."
    
    result = await execute_poster_pipeline(user_input)

    print("\n--- POSTER PIPELINE RESULTS ---")
    if result.get("error"):
        print(f"Error: {result['error']}")
    else:
        # Pretty print the results
        print(json.dumps(result, indent=2))
        
    print("------------------------------------")
    
    # Save the generated artifact to disk to view it
    if not result.get("error") and result.get("artifact_saved") != "No":
        print("\nAttempting to save artifact to disk...")
        try:
            session_id = result["session_id"]
            filename_to_load = result["artifact_saved"]

            # Load the artifact from the service using the session_id
            final_artifact = await _artifact_service.load_artifact(
                app_name=APP_NAME,
                user_id=USER_ID,
                session_id=session_id,
                filename=filename_to_load,
            )

            if final_artifact and final_artifact.inline_data:
                output_filename = "output_image.png"
                # Open a file in binary write mode ('wb') and save the data
                with open(output_filename, "wb") as f:
                    f.write(final_artifact.inline_data.data)
                print(f"✅ Success! Image saved to '{output_filename}'")
            else:
                print("⚠️ Could not retrieve the artifact from the service.")
        except Exception as e:
            print(f"❌ Failed to save artifact to disk. Error: {e}")


if __name__ == "__main__":
    asyncio.run(main())
File: app\services\poster_agent\__init__.py
from . import agent
File: app\services\root_agent\agent.py
import asyncio
import uuid
import os
from typing import Dict, Any

from google.adk.agents.sequential_agent import SequentialAgent
from google.adk.agents.llm_agent import LlmAgent
from google.genai.types import Content, Part
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from dotenv import load_dotenv
from pydantic import BaseModel
from app.services.greeting_agent.agent import execute_greeting
from app.services.bq_agent.agent import execute_sql_pipeline
from app.services.visualization_agent.agent import execute_visualization_pipeline
from app.services.poster_agent.agent import execute_poster_pipeline




# --- Constants ---
APP_NAME = "code_pipeline_module_app"
USER_ID = "dev_user_01"
# Make sure to use a valid model available to you.
GEMINI_MODEL = "gemini-2.0-flash-001"

# --- Pydantic Models for Agent Outputs ---
class CodeWriterOutput(BaseModel):
    code: str
    code_explanation: str

class CodeReviewerOutput(BaseModel):
    code: str  # The code that was reviewed
    code_review: str

# --- Agent and Session Service Setup (Instantiated once on import) ---

# Code Writer Agent
_code_writer_agent = LlmAgent(
    name="CodeWriterAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Writer AI.
    Based on the user's request, write the initial Python code and provide a brief explanation.
    Output your response as a JSON object with two keys: 'code' (the Python code block) and 'code_explanation' (a brief explanation of the code).
    """,
    output_key="generated_code_data",
    output_schema=CodeWriterOutput,
)

# Code Reviewer Agent
_code_reviewer_agent = LlmAgent(
    name="CodeReviewerAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Reviewer AI.
    You will be provided with Python code. Review it and provide constructive feedback.
    Output your response as a JSON object with two keys: 'code' (the exact Python code you reviewed) and 'code_review' (your feedback).
    Focus on clarity, correctness, potential errors, style issues, or improvements.
    """,
    output_key="review_data",
    output_schema=CodeReviewerOutput,
)

# Code Refactorer Agent
_code_refactorer_agent = LlmAgent(
    name="CodeRefactorerAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Refactorer AI.
    You will be provided with original Python code and review comments.
    Refactor the original code to address the feedback and improve its quality.
    Output your response as a JSON object with two keys: 'code' (the final, refactored Python code block) and 'code_explanation' (a brief explanation of the changes made).
    """,
    output_key="refactored_code_data",
    output_schema=CodeWriterOutput,
)

# The Sequential Agent Pipeline
_code_pipeline_agent = SequentialAgent(
    name="CodePipelineAgent",
    sub_agents=[_code_writer_agent, _code_reviewer_agent, _code_refactorer_agent],
)

# Session Service
_session_service = InMemorySessionService()


# ==============================================================================
# CORRECTED FUNCTION
# ==============================================================================
async def execute_code_pipeline(user_query: str) -> Dict[str, Any]:
    """
    Executes the complete Code Generation, Review, and Refactoring pipeline asynchronously.

    This function leverages an AI-powered pipeline to generate code based on a user's query,
    review the generated code, and refactor it for improvements. It ensures that the process
    is managed within a session and returns comprehensive results from each stage of the pipeline.

    Args:
        user_query (str): A string describing the desired functionality of the code.
                          Example: "a function to calculate the nth Fibonacci number".

    Returns:
        dict: A dictionary containing the results from each stage of the pipeline:
            - 'initial_code' (str): The code generated initially.
            - 'initial_code_explanation' (str): Explanation of the initially generated code.
            - 'reviewed_code_input' (str): The code that was reviewed.
            - 'review_comments' (str): Comments and feedback from the code review.
            - 'refactored_code' (str): The refactored version of the code.
            - 'refactored_code_explanation' (str): Explanation of the refactored code.
            - 'error' (str, optional): An error message if the pipeline fails at any stage.
    """
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        print(f"▶️  Running AI pipeline for query: '{user_query[:50]}...'")
        current_session_id = str(uuid.uuid4())

        # Directly use 'await' since we are in an async function
        await _session_service.create_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        runner = Runner(
            agent=_code_pipeline_agent,
            app_name=APP_NAME,
            session_service=_session_service,
        )

        initial_message = Content(role="user", parts=[Part(text=user_query)])

        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass  # We just need to let the runner complete

        session_state_data = await _session_service.get_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # The ADK now correctly parses Pydantic models into dicts in the state
        initial_code_output = session_state_data.state.get("generated_code_data")
        review_output = session_state_data.state.get("review_data")
        refactored_code_output = session_state_data.state.get("refactored_code_data")

        print("✅ Pipeline completed successfully.")
        return {
            "initial_code": initial_code_output.get("code") if initial_code_output else "Error: Initial code not generated.",
            "initial_code_explanation": initial_code_output.get("code_explanation") if initial_code_output else "No explanation provided.",
            "reviewed_code_input": review_output.get("code") if review_output else "Error: Code for review not captured.",
            "review_comments": review_output.get("code_review") if review_output else "Error: Review not generated.",
            "refactored_code": refactored_code_output.get("code") if refactored_code_output else "Error: Code not refactored.",
            "refactored_code_explanation": refactored_code_output.get("code_explanation") if refactored_code_output else "No explanation provided.",
        }

    except Exception as e:
        print(f"❌ Pipeline failed with an error: {e}")
        # Print the full traceback for better debugging
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "initial_code": "Pipeline failed.",
            "initial_code_explanation": "Pipeline failed.",
            "reviewed_code_input": "Pipeline failed.",
            "review_comments": "Pipeline failed.",
            "refactored_code": "Pipeline failed.",
            "refactored_code_explanation": "Pipeline failed.",
        }
    

def add(a: int, b: int) -> dict:
    """Adds two numbers together.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the addition.
    """
    return {
        "status": "success",
        "result": a + b
    }

def subtract(a: int, b: int) -> dict:
    """Subtracts the second number from the first number.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the subtraction.
    """
    return {
        "status": "success",
        "result": a - b
    }

def multiply(a: int, b: int) -> dict:
    """Multiplies two numbers together.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the multiplication.
    """
    return {
        "status": "success",
        "result": a * b
    }


import datetime
from zoneinfo import ZoneInfo
def get_weather(city: str) -> dict:
    """Retrieves the current weather report for a specified city.

    Args:
        city (str): The name of the city for which to retrieve the weather report.

    Returns:
        dict: status and result or error msg.
    """
    if city.lower() == "new york":
        return {
            "status": "success",
            "report": (
                "The weather in New York is sunny with a temperature of 25 degrees"
                " Celsius (77 degrees Fahrenheit)."
            ),
        }
    else:
        return {
            "status": "error",
            "error_message": f"Weather information for '{city}' is not available.",
        }


def get_current_time(city: str) -> dict:
    """Returns the current time in a specified city.

    Args:
        city (str): The name of the city for which to retrieve the current time.

    Returns:
        dict: status and result or error msg.
    """

    if city.lower() == "new york":
        tz_identifier = "America/New_York"
    else:
        return {
            "status": "error",
            "error_message": (
                f"Sorry, I don't have timezone information for {city}."
            ),
        }

    tz = ZoneInfo(tz_identifier)
    now = datetime.datetime.now(tz)
    report = (
        f'The current time in {city} is {now.strftime("%Y-%m-%d %H:%M:%S %Z%z")}'
    )
    return {"status": "success", "report": report}

from typing import AsyncGenerator
async def monitor_stock_price(stock_symbol: str) -> AsyncGenerator[str, None]:
  """This function will monitor the price for the given stock_symbol in a continuous, streaming and asynchronously way."""
  print(f"Start monitor stock price for {stock_symbol}!")

  # Let's mock stock price change.
  await asyncio.sleep(4)
  price_alert1 = f"the price for {stock_symbol} is 300"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(4)
  price_alert1 = f"the price for {stock_symbol} is 400"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(20)
  price_alert1 = f"the price for {stock_symbol} is 900"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(20)
  price_alert1 = f"the price for {stock_symbol} is 500"
  yield price_alert1
  print(price_alert1)

# from google.adk.agents import LlmAgent, Function
from google.adk.tools.agent_tool import AgentTool


supervisor = LlmAgent(
    name="Supervisor",
    model="gemini-2.0-flash-live-001", # Use a consistent and available model
    instruction="""
    You are a smart agent responsible for directing user requests to the appropriate sub-agents. Your main role is to evaluate the user's query and assign it to the correct sub-agent.

    Upon receiving a user query, you must analyze its content to determine which sub-agent tool should handle it. You have the following tools available for different types of operations:

    execute_greeting tool
    Add tool
    Subtract tool
    Multiply tool
    Get current time tool
    Get weather tool
    execute_sql_pipeline: for generating and executing SQL queries and returning results.
    execute_visualization_pipeline: for creating graph visualization of the data based on user requests.
    execute_poster_pipeline: for creating posters or images based on user requests.

    Your task is to assess the user’s query and decide which tool is best suited. 
    After calling a tool, you MUST return the result of that tool's execution to the user. 
    For example, if a visualization is created, confirm that the chart has been generated.
    """,
    description="Intelligent router for directing user queries to the appropriate sub-agent tools.",
    tools=[add,subtract,multiply,get_current_time,get_weather,execute_greeting,execute_sql_pipeline,execute_visualization_pipeline,execute_poster_pipeline],  # This part is correct
)

#monitor_stock_price tool
#execute_code_pipeline
File: app\services\root_agent\agent_legacy.py
import asyncio
import uuid
import os
from typing import Dict, Any

from google.adk.agents.sequential_agent import SequentialAgent
from google.adk.agents.llm_agent import LlmAgent
from google.genai.types import Content, Part
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from dotenv import load_dotenv
from pydantic import BaseModel

# --- Constants ---
APP_NAME = "code_pipeline_module_app"
USER_ID = "dev_user_01"
# Make sure to use a valid model available to you.
GEMINI_MODEL = "gemini-2.0-flash-001"

# --- Pydantic Models for Agent Outputs ---
class CodeWriterOutput(BaseModel):
    code: str
    code_explanation: str

class CodeReviewerOutput(BaseModel):
    code: str  # The code that was reviewed
    code_review: str

# --- Agent and Session Service Setup (Instantiated once on import) ---

# Code Writer Agent
_code_writer_agent = LlmAgent(
    name="CodeWriterAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Writer AI.
    Based on the user's request, write the initial Python code and provide a brief explanation.
    Output your response as a JSON object with two keys: 'code' (the Python code block) and 'code_explanation' (a brief explanation of the code).
    """,
    output_key="generated_code_data",
    output_schema=CodeWriterOutput,
)

# Code Reviewer Agent
_code_reviewer_agent = LlmAgent(
    name="CodeReviewerAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Reviewer AI.
    You will be provided with Python code. Review it and provide constructive feedback.
    Output your response as a JSON object with two keys: 'code' (the exact Python code you reviewed) and 'code_review' (your feedback).
    Focus on clarity, correctness, potential errors, style issues, or improvements.
    """,
    output_key="review_data",
    output_schema=CodeReviewerOutput,
)

# Code Refactorer Agent
_code_refactorer_agent = LlmAgent(
    name="CodeRefactorerAgent",
    model=GEMINI_MODEL,
    instruction="""You are a Code Refactorer AI.
    You will be provided with original Python code and review comments.
    Refactor the original code to address the feedback and improve its quality.
    Output your response as a JSON object with two keys: 'code' (the final, refactored Python code block) and 'code_explanation' (a brief explanation of the changes made).
    """,
    output_key="refactored_code_data",
    output_schema=CodeWriterOutput,
)

# The Sequential Agent Pipeline
_code_pipeline_agent = SequentialAgent(
    name="CodePipelineAgent",
    sub_agents=[_code_writer_agent, _code_reviewer_agent, _code_refactorer_agent],
)

# Session Service
_session_service = InMemorySessionService()


# ==============================================================================
# CORRECTED FUNCTION
# ==============================================================================
async def execute_code_pipeline(user_query: str) -> Dict[str, Any]:
    """
    Executes the complete Code Generation, Review, and Refactoring pipeline asynchronously.

    This function leverages an AI-powered pipeline to generate code based on a user's query,
    review the generated code, and refactor it for improvements. It ensures that the process
    is managed within a session and returns comprehensive results from each stage of the pipeline.

    Args:
        user_query (str): A string describing the desired functionality of the code.
                          Example: "a function to calculate the nth Fibonacci number".

    Returns:
        dict: A dictionary containing the results from each stage of the pipeline:
            - 'initial_code' (str): The code generated initially.
            - 'initial_code_explanation' (str): Explanation of the initially generated code.
            - 'reviewed_code_input' (str): The code that was reviewed.
            - 'review_comments' (str): Comments and feedback from the code review.
            - 'refactored_code' (str): The refactored version of the code.
            - 'refactored_code_explanation' (str): Explanation of the refactored code.
            - 'error' (str, optional): An error message if the pipeline fails at any stage.
    """
    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        print(f"▶️  Running AI pipeline for query: '{user_query[:50]}...'")
        current_session_id = str(uuid.uuid4())

        # Directly use 'await' since we are in an async function
        await _session_service.create_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        runner = Runner(
            agent=_code_pipeline_agent,
            app_name=APP_NAME,
            session_service=_session_service,
        )

        initial_message = Content(role="user", parts=[Part(text=user_query)])

        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass  # We just need to let the runner complete

        session_state_data = await _session_service.get_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # The ADK now correctly parses Pydantic models into dicts in the state
        initial_code_output = session_state_data.state.get("generated_code_data")
        review_output = session_state_data.state.get("review_data")
        refactored_code_output = session_state_data.state.get("refactored_code_data")

        print("✅ Pipeline completed successfully.")
        return {
            "initial_code": initial_code_output.get("code") if initial_code_output else "Error: Initial code not generated.",
            "initial_code_explanation": initial_code_output.get("code_explanation") if initial_code_output else "No explanation provided.",
            "reviewed_code_input": review_output.get("code") if review_output else "Error: Code for review not captured.",
            "review_comments": review_output.get("code_review") if review_output else "Error: Review not generated.",
            "refactored_code": refactored_code_output.get("code") if refactored_code_output else "Error: Code not refactored.",
            "refactored_code_explanation": refactored_code_output.get("code_explanation") if refactored_code_output else "No explanation provided.",
        }

    except Exception as e:
        print(f"❌ Pipeline failed with an error: {e}")
        # Print the full traceback for better debugging
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "initial_code": "Pipeline failed.",
            "initial_code_explanation": "Pipeline failed.",
            "reviewed_code_input": "Pipeline failed.",
            "review_comments": "Pipeline failed.",
            "refactored_code": "Pipeline failed.",
            "refactored_code_explanation": "Pipeline failed.",
        }
    

def add(a: int, b: int) -> dict:
    """Adds two numbers together.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the addition.
    """
    return {
        "status": "success",
        "result": a + b
    }

def subtract(a: int, b: int) -> dict:
    """Subtracts the second number from the first number.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the subtraction.
    """
    return {
        "status": "success",
        "result": a - b
    }

def multiply(a: int, b: int) -> dict:
    """Multiplies two numbers together.

    Args:
        a (int): The first number.
        b (int): The second number.

    Returns:
        dict: The result of the multiplication.
    """
    return {
        "status": "success",
        "result": a * b
    }


import datetime
from zoneinfo import ZoneInfo
def get_weather(city: str) -> dict:
    """Retrieves the current weather report for a specified city.

    Args:
        city (str): The name of the city for which to retrieve the weather report.

    Returns:
        dict: status and result or error msg.
    """
    if city.lower() == "new york":
        return {
            "status": "success",
            "report": (
                "The weather in New York is sunny with a temperature of 25 degrees"
                " Celsius (77 degrees Fahrenheit)."
            ),
        }
    else:
        return {
            "status": "error",
            "error_message": f"Weather information for '{city}' is not available.",
        }


def get_current_time(city: str) -> dict:
    """Returns the current time in a specified city.

    Args:
        city (str): The name of the city for which to retrieve the current time.

    Returns:
        dict: status and result or error msg.
    """

    if city.lower() == "new york":
        tz_identifier = "America/New_York"
    else:
        return {
            "status": "error",
            "error_message": (
                f"Sorry, I don't have timezone information for {city}."
            ),
        }

    tz = ZoneInfo(tz_identifier)
    now = datetime.datetime.now(tz)
    report = (
        f'The current time in {city} is {now.strftime("%Y-%m-%d %H:%M:%S %Z%z")}'
    )
    return {"status": "success", "report": report}

from typing import AsyncGenerator
async def monitor_stock_price(stock_symbol: str) -> AsyncGenerator[str, None]:
  """This function will monitor the price for the given stock_symbol in a continuous, streaming and asynchronously way."""
  print(f"Start monitor stock price for {stock_symbol}!")

  # Let's mock stock price change.
  await asyncio.sleep(4)
  price_alert1 = f"the price for {stock_symbol} is 300"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(4)
  price_alert1 = f"the price for {stock_symbol} is 400"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(20)
  price_alert1 = f"the price for {stock_symbol} is 900"
  yield price_alert1
  print(price_alert1)

  await asyncio.sleep(20)
  price_alert1 = f"the price for {stock_symbol} is 500"
  yield price_alert1
  print(price_alert1)

supervisor = LlmAgent(
    name="Supervisor",
    model="gemini-2.0-flash-live-001", # Use a consistent and available model
    instruction="""
    You are a smart agent responsible for directing user requests to the appropriate sub-agents. Your main role is to evaluate the user's query and assign it to the correct sub-agent.

    Upon receiving a user query, you must analyze its content to determine which sub-agent tool should handle it. You have the following tools available for different types of operations:

    Add tool
    Subtract tool
    Multiply tool
    Get current time tool
    Get weather tool
    Execute code pipeline tool
    monitor_stock_price tool
Your task is to assess the user’s query and decide which tool is best suited for processing it.
    """,
    description="Intelligent router for directing user queries to the appropriate sub-agent tools.",
    tools=[add,subtract,multiply,get_current_time,get_weather,execute_code_pipeline,monitor_stock_price],  # This part is correct
)
File: app\services\root_agent\__init__.py
from . import agent_legacy
File: app\services\test_agent\agent.py
from google.adk.agents import LlmAgent, LoopAgent, SequentialAgent
from google.adk.agents.callback_context import CallbackContext
from google.cloud import bigquery
from google.cloud.exceptions import NotFound, GoogleCloudError
import logging
import traceback
import os
MODEL_GEMINI_2_0_FLASH="gemini-2.0-flash-live-001"

# MODEL_GEMINI_2_0_FLASH="gemini-2.0-flash-001"  # Use the latest flash model available

logging.basicConfig(
    level=logging.INFO,  # Changed to INFO for better visibility of service operations
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

def initialize_state_var(callback_context: CallbackContext):
    PROJECT = "hackathon-agents"  # Default project ID, can be overridden by environment variable
    BQ_LOCATION = "us-central1"
    DATASET =  "StyleHub"

    callback_context.state["PROJECT"] = PROJECT
    callback_context.state["BQ_LOCATION"] = BQ_LOCATION
    callback_context.state["DATASET"] =DATASET

    bigquery_metadata = bigquery_metdata_extraction_tool()

    callback_context.state["bigquery_metadata"] = bigquery_metadata

class BigQueryReader:
    """
    A class to encapsulate BigQuery read operations using a service account.
    Focuses on querying public datasets.
    """

    def __init__(self, project_id: str, service_account_key_path: str):
        """
        Initializes the BigQuery client.

        Args:
            project_id (str): Your Google Cloud Project ID. This is required for billing
                              and job execution context, even for public datasets.
            service_account_key_path (str): Path to your service account JSON key file.
        """
        if not os.path.exists(service_account_key_path):
            logger.error(
                f"Service account key file not found at: {service_account_key_path}"
            )
            raise FileNotFoundError(
                f"Service account key file not found at: {service_account_key_path}"
            )

        self.project_id = project_id
        self.service_account_key_path = service_account_key_path
        self.client = None
        self._initialize_client()
        logger.info(f"BigQueryReader initialized for project: {self.project_id}")

    def _initialize_client(self):
        """
        Internal method to set up the BigQuery client.
        Sets GOOGLE_APPLICATION_CREDENTIALS environment variable.
        """
        try:
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self.service_account_key_path
            self.client = bigquery.Client(project=self.project_id)
            # Test connection by making a small request
            # self.client.list_projects(max_results=1) # A simple test if needed
            logger.info(
                f"BigQuery client successfully initialized for project: {self.client.project}"
            )
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client: {e}")
            raise ConnectionError(
                f"Could not connect to BigQuery. Check credentials and project ID. Error: {e}"
            )

    def list_tables_in_dataset(
        self, project: str = 'bigquery-public-data', dataset_id: str= 'thelook_ecommerce', max_results: int = 10
    ) -> list:
        """
        Lists tables in a specified BigQuery dataset.
        Default project is bigquery-public-data and the default dataset is thelook_ecommerce.

        Args:
            project (str): The project ID where the dataset resides (e.g., 'bigquery-public-data').
            dataset_id (str): The ID of the dataset (e.g., 'thelook_ecommerce').
            max_results (int): Maximum number of tables to retrieve.

        Returns:
            list: A list of table IDs (strings). Returns an empty list on error.
        """
        logger.info(f"Attempting to list tables in '{project}.{dataset_id}'...")
        try:
            dataset_ref = bigquery.DatasetReference(project, dataset_id)
            tables = []
            for table_item in self.client.list_tables(
                dataset_ref, max_results=max_results
            ):
                tables.append(table_item.table_id)
            logger.info(
                f"Successfully listed {len(tables)} tables in '{project}.{dataset_id}'."
            )
            return tables
        except NotFound:
            logger.warning(
                f"Dataset '{project}.{dataset_id}' not found or inaccessible."
            )
            return []
        except GoogleCloudError as e:
            logger.error(
                f"Google Cloud Error listing tables in '{project}.{dataset_id}': {e}"
            )
            return []
        except Exception as e:
            logger.error(f"An unexpected error occurred while listing tables: {e}")
            return []

    def execute_query(self, query: str) -> tuple:
        """
        Executes a SQL query on BigQuery and returns the results.
        Default project is `bigquery-public-data` and the default dataset is `thelook_ecommerce`.

        Args:
            query (str): The SQL query string to execute.

        Returns:
            tuple: The SQL query and a list of BigQuery Row objects, or an empty list if an error occurs.
        """
        logger.info("Executing BigQuery query...")
        try:
            print("Generated sql:> ",query)
            query_job = self.client.query(query)  # API request
            results = query_job.result()  # Waits for job to complete
            rows = [
                dict(row) for row in results
            ]  # Convert rows to dictionaries for easier handling
            logger.info(f"Query executed successfully. Fetched {len(rows)} rows.")
            return (rows)
        except Exception as e:
            error_message = f"An unexpected error occurred during query execution: {e}"
            logger.error(error_message)
            logger.error(traceback.format_exc())
        return (traceback.format_exc())

bq_reader = BigQueryReader(
            project_id="hackathon-agents",
            service_account_key_path=r"D:\3_hackathon\1_llm_agent_hackathon_google\cautious-palm-tree\hackathon-agents-044c975e8972.json"
        )

import json
def json_to_paragraphs(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
        
    paragraphs = []
    
    for table in data.get('tables', []):
        table_name = table.get('table_name', 'Unnamed Table')
        table_description = table.get('table_description', 'No description available.')
        
        paragraph = f"Table '{table_name}': {table_description}\n"
        paragraph += "Columns:\n"
        
        for column in table.get('columns', []):
            column_name = column.get('column_name', 'Unnamed Column')
            column_type = column.get('column_type', 'Unknown Type')
            column_description = column.get('column_description', 'No description available.')
            is_primary_key = column.get('is_primary_key', False)
            primary_key_info = " (Primary Key)" if is_primary_key else ""
            
            foreign_key_info = ""
            if 'foreign_key' in column:
                fk_table = column['foreign_key'].get('reference_table', 'Unknown Table')
                fk_column = column['foreign_key'].get('reference_column', 'Unknown Column')
                foreign_key_info = f" (Foreign Key references {fk_table}.{fk_column})"
                
            paragraph += f"  - {column_name} ({column_type}): {column_description}{primary_key_info}{foreign_key_info}\n"
        
        paragraphs.append(paragraph)
    
    return paragraphs


# Agent Prompt pasted here for easy reference
QUERY_UNDERSTANDING_PROMPT_STR = """
    You are playing a data analyst role whose role is to understand the user query provided natural language text query.
    The intention is to identify the bigquery tables and columns that will be needed to answer the query query.
    If the user query is ambiguous, ask for clarifying queries.

    Use the below bigquery metadata which provides the details on tables, columns, data types and descriptions for identifying the tables/columns.
    {bigquery_metadata}

    Format the output in form of JSON with key as table.column and value as reasoning for picking the column.
"""
# LLM Agent for analysis of the user query to identify the user question and derive tables/columns involved 
query_understanding_agent = LlmAgent(
    name = "query_understanding_agent",
    model = "gemini-2.5-flash-preview-04-17",
    description = """This agent is responsible for understanding the intent of the user question 
        and identifying tables/columns involved to answer the query
    """,
    instruction = QUERY_UNDERSTANDING_PROMPT_STR,
    output_key = "query_understanding_output"
)

# Agent Prompt pasted here for easy reference
QUERY_GENERATION_INSTRUCTION_STR = """
    You are playing role of bigquery sql writer.
    Your job is write bigquery sqls in standard dialect.
    
    - Use the analysis done by the query understanding agent as below
      {query_understanding_output}

    - Use the project as {PROJECT}, location as {BQ_LOCATION}, dataset as {DATASET} for generating the bigquery queries for the user provided question.
    - Use the following metadata for understanding the tables, columns, datatypes and description of the columns.
    <METADATA START>
    {bigquery_metadata}
    <METADATA END>

    Output only the generated query as text
    """

def bigquery_metdata_extraction_tool():
    """ Extracts BigQuery table metadata from a JSON file and returns it as paragraphs."""
    json_path= r"D:\3_hackathon\1_llm_agent_hackathon_google\cautious-palm-tree\dataset_info.json"
    metadata= json_to_paragraphs(json_path)
    return metadata

# LLM Agent for generation of bigquery based on the analysis received from the query_understanding_agent
query_generation_agent = LlmAgent(
    name = "query_generation_agent",
    model = "gemini-2.5-flash-preview-04-17",
    description = "This agent is responsible for generating bigquery queries in standard sql dialect",
    instruction = QUERY_GENERATION_INSTRUCTION_STR,
    # tools = [bigquery_metdata_extraction_tool],  #<----Possibility of adding a tool to find out similar queries previously provided (Abhinav's idea)
    output_key = "query_generation_output"
)


QUERY_REVIEW_REWRITE_INSTRUCTION_STR = """
    You are playing role of bigquery sql reviewer and rewriter.
    Your job is review and based on the review if any rewrite bigquery sqls in standard dialect.
    
    - Use the query understanding agent output as below
      {query_understanding_output}

    - Use the query generated done by the query generation agent as below
      {query_generation_output}

    - Use the project as {PROJECT}, location as {BQ_LOCATION}, dataset as {DATASET} for generating the bigquery queries for the user provided question.
    - Use the `bigquery_metadata_extraction_tool` for metadata extraction for understanding the tables, columns, datatypes and description of the columns.
    
    Review Items
    - check if the columns have proper aliases, if not added appropriate alias
    - Add limit to 10 in case of select queries that might fetch lot of records
    - check if all columns are needed in query and bring the relevant ones
    - handle the casing of the filter conditions for matching eg: upper(state) = "OHIO" or lower(state)="ohio"
    - convert the datetime attributes to string for display purposes

    Output only the rewritten query as text
    """

# LLM Agent for review of the SQL queries and rewriting the sql queries if needed
query_review_rewrite_agent = LlmAgent(
    name = "query_review_agent",
    model = "gemini-2.5-flash-preview-04-17",
    description = f"This agent is responsible for reviewing queries in the bigquery",
    instruction = QUERY_REVIEW_REWRITE_INSTRUCTION_STR,
    output_key = "query_review_rewrite_output"
)

QUERY_EXECUTION_INSTRUCTION_STR = """
    You are playing role of bigquery sql executor.
    Your job is review and based on the review if any rewrite bigquery sqls in standard dialect.
    
    - Execute the query generated below on bigqquery using the `bq_reader.execute_query` and display the results as markdown table with proper headers
      {query_review_rewrite_output}

    """

# LLM Agent for execution of the bigquery sqls
query_execution_agent = LlmAgent(
    name = "query_execution_agent",
    model = "gemini-2.5-flash-preview-04-17",
    description = f"This agent is responsible for exeuction of queries in the bigquery and present the result as markdown table",
    instruction = QUERY_EXECUTION_INSTRUCTION_STR,
    tools = [ bq_reader.execute_query ],
    output_key = "query_execution_output"
)


sql_pipeline_agent = SequentialAgent(
    name="SQLPipelineAgent",
    sub_agents=[query_understanding_agent, query_generation_agent, query_review_rewrite_agent, query_execution_agent],
    description="Executes a sequence of code writing, reviewing, and refactoring.",
    # The agents will run in the order provided: Writer -> Reviewer -> Refactorer
    before_agent_callback=initialize_state_var,
)

#POSTER CREATION AGENT-------------------------------------------------------START>
from google.adk.tools import agent_tool
from google.adk.tools import load_artifacts
from google.adk.tools import ToolContext
from google.genai import Client
from google.genai import types
client = Client()

async def generate_image(prompt: str, tool_context: 'ToolContext'):
  """Generates an image based on the prompt."""
  response = client.models.generate_images(
      model='imagen-3.0-generate-002',
      prompt=prompt,
      config={'number_of_images': 1},
  )
  if not response.generated_images:
    return {'status': 'failed'}
  image_bytes = response.generated_images[0].image.image_bytes
  await tool_context.save_artifact(
      'image.png',
      types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
  )
  return {
      'status': 'success',
      'detail': 'Image generated successfully and stored in artifacts.',
      'filename': 'image.png',
  }
  
image_generator_agent = LlmAgent(
    model=MODEL_GEMINI_2_0_FLASH, #'gemini-2.0-flash-001',
    name='root_agent',
    description="""An agent that generates images and answer questions about the images.""",
    instruction="""You are an agent whose job is to generate or edit an image based on the user's prompt.
""",
    tools=[generate_image, load_artifacts],
)

# image_agent = generate_image()
# image_tool = agent_tool.AgentTool(agent=image_agent)
#POSTER CREATION AGENT-------------------------------------------------------END>

#GREETING AGENT-------------------------------------------------------START>
general_greeting_agent = LlmAgent(
    name="general_greeting_agent",
    model=MODEL_GEMINI_2_0_FLASH, #"gemini-2.5-flash-preview-04-17",
    description=(
        "Agent to answer questions relating to user general query"
    ),
    instruction=(
        """You are a helpful agent who can answer user questions and have a great open conversation.
        You can speak in English, Hindi, or any other language."""
    ),
)

greeting_tool = agent_tool.AgentTool(agent=general_greeting_agent)
#GREETING AGENT-------------------------------------------------------END>

#VISUALIZATION AGENT-------------------------------------------------------START>
chart_type_agent = LlmAgent(
    name="visualization_agent",
    model=MODEL_GEMINI_2_0_FLASH,#"gemini-2.5-flash-preview-04-17", #{query_understanding_output}
    description=(
        "Agent to predict the chart type and the design of the chart based on the user query and the data provided."
    ),
    instruction=(
        """You are a chart type agent who can predict the chart type and the design of the chart based on the user query and the data provided. You will be provided with the User query and the data in the form of a JSON object.
        You need to analyze the data and the user query to predict the chart type and the design of the chart. You can use the following chart types: bar, line, pie, scatter, area, histogram, boxplot, heatmap, radar, treemap, funnel, waterfall, gauge, bullet, polar, sunburst, chord, sankey.
        #User Query: "Users who ordered the most"
        #Data: ```{query_execution_output}```

        """
    ),
    output_key="chart_type_output",
)

plotly_code_agent = LlmAgent(
    name="plotly_code_agent",
    model=MODEL_GEMINI_2_0_FLASH,#"gemini-2.5-flash-preview-04-17",
    description=(
        "Agent to generate plotly code for the chart type and design predicted by the chart type agent."
    ),
    instruction=(
        """You are a plotly code agent who can generate plotly code for the chart type and design predicted by the chart type agent. You will be provided with the chart type and the design of the chart.
        You need to generate the plotly code for the chart type and the design of the chart. 
        The final plotly code should save the chart as a PNG image and return the image path.
        #Chart Type: ```{chart_type_output}```
        #Data: ```{query_execution_output}```
"""
),
    output_key="plotly_code_output",
)

    
import plotly.graph_objects as go
import plotly.io as pio

async def execute_plotly_code_and_get_image_bytes(plotly_code_str: str,tool_context: 'ToolContext'):
    # Create a local dictionary to execute the code
    local_vars = {}
    # Execute the Plotly code string
    exec(plotly_code_str, {}, local_vars)
    # Extract the figure object
    fig = local_vars.get('fig')
    if fig is None or not isinstance(fig, go.Figure):
        raise ValueError("The Plotly code must define a Plotly figure object named 'fig'.")
    # Get the image bytes
    image_bytes = pio.to_image(fig, format='png')
    await tool_context.save_artifact(
      'plot.png',
      types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
  )
    return {
      'status': 'success',
      'detail': 'Image generated successfully and stored in artifacts.',
      'filename': 'plot.png',
  }

plotly_code_executor_agent = LlmAgent(
    model=MODEL_GEMINI_2_0_FLASH,#'gemini-2.0-flash-001',
    name='plotly_code_executor_agent',
    description="""An agent that executes Plotly code and generates an image from it.""",
    instruction="""Use `execute_plotly_code_and_get_image_bytes` tool to execute the Plotly code and generate an image.
    
""",
    tools=[execute_plotly_code_and_get_image_bytes, load_artifacts],
)

visualization_agent = SequentialAgent(
    name="visualization_agent",
    sub_agents=[chart_type_agent, plotly_code_agent, plotly_code_executor_agent],
    description="Executes a sequence of chart type prediction, plotly code generation, and image generation.",
    # instruction="""This agent is responsible for generating visualizations based on user queries."""
    # The agents will run in the order provided: Chart Type -> Plotly Code -> Image Generation
    # before_agent_callback=initialize_state_var,
)



# Below is a working example but fails in authentication:> 
from google.adk.tools.application_integration_tool.application_integration_toolset import ApplicationIntegrationToolset
sa_key_file_path =r"D:\3_hackathon\1_llm_agent_hackathon_google\cautious-palm-tree\rough_work_scripts\hackathon-agents-f18a9f8dc92b.json"

# Read the entire file content into a string
with open(sa_key_file_path, 'r') as f:
    sa_key_string = f.read()
    print("key:> ",sa_key_string)



email_tool = ApplicationIntegrationToolset(
    project="hackathon-agents", # TODO: replace with GCP project of the connection
    location="us-central1", #TODO: replace with location of the connection
    integration="sendEmailAshish", #TODO: replace with integration name
    triggers=["api_trigger/sendEmailAshish_API_1"],#TODO: replace with trigger id(s). Empty list would mean all api triggers in the integration to be considered. 
    service_account_json=sa_key_string, #optional. Stringified json for service account key
    tool_name_prefix="send email",
    tool_instructions="Use this tool to send email using the integration",
)

email_agent = LlmAgent(
    model=MODEL_GEMINI_2_0_FLASH,#'gemini-2.0-flash',
    name='connector_agent',
    instruction="Use `email_tool` to send emails.",
    tools=[email_tool],
)

# coordinator = LlmAgent(
#     name="HelpDeskCoordinator",
#     model=MODEL_GEMINI_2_0_FLASH,#"gemini-2.5-flash-preview-04-17",
#     instruction="""
#     You are an intelligent agent who routes the requests to the appropriate sub-agents based on the user query.
#     Your primary task is to route the requests to the appropriate sub-agents based on the user query.
#     You will receive a user query and you need to analyze it to determine the best sub-agent to handle it.
#     You can use the following sub-agents to handle different types of requests:
#     'sql_pipeline_agent': Handles SQL query generation, review, and execution.
#     'image_generator_agent' tool: Handles image generation requests.
#     'greeting_tool': Handles general user queries and greetings.
#     'email_agent': Handles email sending requests from the user.
#     'visualization_agent': Handles data visualization requests.
#     You should analyze the user query and determine which sub-agent is best suited to handle it.
    
#     """,
#     description="Main Customer data platform(CDP) help desk router.",
#     tools = [greeting_tool],
#     sub_agents=[sql_pipeline_agent, image_generator_agent, visualization_agent, email_agent]
# )

sql_pipeline_agent_tool = agent_tool.AgentTool(agent=sql_pipeline_agent)
image_generator_agent_tool = agent_tool.AgentTool(agent=image_generator_agent)
visualization_agent_tool = agent_tool.AgentTool(agent=visualization_agent)
email_agent_tool = agent_tool.AgentTool(agent=email_agent)

coordinator1 = LlmAgent(
    name="HelpDeskCoordinator",
    model=MODEL_GEMINI_2_0_FLASH,#"gemini-2.5-flash-preview-04-17",
    instruction="""
    You are an intelligent agent who routes the requests to the appropriate sub-agents based on the user query.
    Your primary task is to route the requests to the appropriate sub-agents based on the user query.
    You will receive a user query and you need to analyze it to determine the best sub-agent to handle it.
    You can use the following sub-agents to handle different types of requests:
    'sql_pipeline_agent': Handles SQL query generation, review, and execution.
    'image_generator_agent' tool: Handles image generation requests.
    'greeting_tool': Handles general user queries and greetings.
    'email_agent': Handles email sending requests from the user.
    'visualization_agent': Handles data visualization requests.
    You should analyze the user query and determine which sub-agent is best suited to handle it.
    
    """,
    description="Main Customer data platform(CDP) help desk router.",
    tools = [greeting_tool,sql_pipeline_agent_tool,image_generator_agent_tool,visualization_agent_tool,email_agent_tool],
    # sub_agents=[sql_pipeline_agent, image_generator_agent, visualization_agent, email_agent]
)


# For ADK tools compatibility, the root agent must be named `root_agent`
root_agent = coordinator1 #<--------------------------------------important to note that this is the root agent for ADK tools compatibility
File: app\services\test_agent\__init__.py

File: app\services\test_agent\__main__.py

File: app\services\visualization_agent\agent.py
import asyncio
import uuid
import os
import json
from typing import Dict, Any, List

from google.adk.agents import LlmAgent, SequentialAgent
from google.adk.sessions import InMemorySessionService
from google.adk.artifacts import InMemoryArtifactService
from google.adk.runners import Runner
from google.genai.types import Content, Part
from dotenv import load_dotenv
import logging
import traceback

# Import Plotly for the tool
import plotly.graph_objects as go
import plotly.io as pio
# Import ADK tool-related classes
from google.adk.tools import ToolContext
from google.genai import types

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Constants ---
APP_NAME = "visualization_app"
USER_ID = "dev_user_01"
MODEL_GEMINI_2_0_FLASH = "gemini-2.0-flash-001"

# ==============================================================================
# AGENT AND TOOL DEFINITIONS
# ==============================================================================

# Agent 1: Predicts the best chart type (Unchanged)
chart_type_agent = LlmAgent(
    name="chart_type_predictor_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    description="Predicts the chart type and design based on the user query and provided data.",
    instruction=(
        """You are an expert data visualization AI. Your task is to predict the most effective chart type
        based on a user's query and the data they provide.

        Analyze the structure of the data (column names, number of rows) and the user's goal.
        Recommend a chart type (e.g., 'bar', 'line', 'pie', 'scatter') and provide a brief justification.

        User Query: "{user_query}"
        Data (first 5 rows): ```{query_execution_output}```

        Output your prediction as a JSON object with keys "chart_type" and "justification".
        """
    ),
    output_key="chart_type_output",
)

# Agent 2: Generates Plotly code (Unchanged)
plotly_code_agent = LlmAgent(
    name="plotly_code_generator_agent",
    model=MODEL_GEMINI_2_0_FLASH,
    description="Generates Python Plotly code for the predicted chart type and data.",
    instruction=(
        """You are a Python Plotly expert. Your task is to write Plotly code to generate a chart.

        - The data is available in a variable named `data`, which is a list of dictionaries.
        - The chart specification is provided below.
        - The generated Python code must create a Plotly Figure object and assign it to a variable named `fig`.
        - Do not include any `import` statements or data loading code. Assume `data` is pre-loaded.
        - Ensure the chart has clear titles and axis labels.

        Chart Specification: ```{chart_type_output}```
        Data for Charting: ```{query_execution_output}```

        Output ONLY the raw Python code required to generate the figure.
        """
    ),
    output_key="plotly_code_output",
)


# ==============================================================================
# CORRECTED TOOL AND EXECUTOR AGENT
# ==============================================================================

# Tool: Executes Plotly code. The signature is now simplified.
async def execute_plotly_code_and_get_image_bytes(
    plotly_code_str: str, tool_context: ToolContext
):
    """
    Executes a string of Plotly Python code to generate and save a chart image.
    The code string must define a variable `fig` holding the Plotly Figure object.
    The data for the chart is retrieved from the session state key 'query_execution_output'.
    """
    try:
        logger.info("Executing generated Plotly code...")
        
        # FIX: Retrieve data from the context instead of as a parameter
        data = tool_context.state.get("query_execution_output")
        if not data:
            raise ValueError("Data not found in session state under key 'query_execution_output'.")

        # Prepare the execution environment with the data
        execution_globals = {'go': go, 'data': data}
        local_vars = {}

        # Execute the code string in the prepared environment
        exec(plotly_code_str, execution_globals, local_vars)

        fig = local_vars.get('fig')
        if fig is None or not isinstance(fig, go.Figure):
            raise ValueError("Plotly code must define a Figure object named 'fig'.")

        logger.info("Generating PNG image from Plotly figure.")
        image_bytes = pio.to_image(fig, format='png')
        
        artifact_filename = "plot.png"
        await tool_context.save_artifact(
            artifact_filename,
            types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
        )
        logger.info(f"Successfully saved chart as artifact: '{artifact_filename}'")
        
        return {
            "status": "success",
            "detail": f"Image generated successfully and stored as artifact '{artifact_filename}'.",
            "filename": artifact_filename,
        }
    except Exception as e:
        error_message = f"Error executing Plotly code: {e}\n{traceback.format_exc()}"
        logger.error(error_message)
        return {"status": "error", "detail": error_message}


# Agent 3: Executes the code using the tool. Instruction is now simplified.
plotly_code_executor_agent = LlmAgent(
    model=MODEL_GEMINI_2_0_FLASH,
    name='plotly_code_executor_agent',
    description="An agent that executes Plotly code to generate an image.",
    # FIX: Simplified instruction, as data is no longer a direct parameter
    instruction="""You are a code execution agent.
    Your task is to execute the provided Plotly code using the `execute_plotly_code_and_get_image_bytes` tool.

    The code to execute is:
    ```{plotly_code_output}```

    Call the tool with the `plotly_code_str` argument. The data is available to the tool automatically.
    """,
    tools=[execute_plotly_code_and_get_image_bytes],
    output_key="execution_summary"
)

# ==============================================================================
# SEQUENTIAL AGENT AND EXECUTION WRAPPER
# ==============================================================================

# The complete sequential visualization pipeline (Unchanged)
visualization_agent = SequentialAgent(
    name="VisualizationPipelineAgent",
    sub_agents=[chart_type_agent, plotly_code_agent, plotly_code_executor_agent],
    description="Generates a chart from data by predicting type, writing code, and executing it.",
)

# Instantiate services once on import
_session_service = InMemorySessionService()
_artifact_service = InMemoryArtifactService()

# The main execution function
async def execute_visualization_pipeline(user_query: str, query_data: List[Dict[str, Any]], tool_context: ToolContext) -> Dict[str, Any]:
    """
    Executes the visualization pipeline to generate a chart image artifact.

    Args:
        user_query (str): The user's natural language request for a visualization.
        query_data (List[Dict[str, Any]]): The data returned from the SQL query pipeline.

    Returns:
        dict: A dictionary containing results from each stage and the final artifact info.
    """
    tool_context.actions.skip_summarization = True
    logger.info("Set skip_summarization=True for visualization pipeline.")

    load_dotenv()
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY environment variable not found."}

    try:
        current_session_id = str(uuid.uuid4())
        print(f"▶️  Running Visualization pipeline for query: '{user_query[:50]}...'")

        # Set the initial state with the data from the previous (SQL) pipeline
        initial_state = {"query_execution_output": query_data, "user_query": user_query}

        await _session_service.create_session(
            app_name=APP_NAME,
            user_id=USER_ID,
            session_id=current_session_id,
            state=initial_state,
        )

        # The Runner is initialized with the artifact_service.
        runner = Runner(
            agent=visualization_agent,
            app_name=APP_NAME,
            session_service=_session_service,
            artifact_service=_artifact_service,
        )

        initial_message = Content(role="user", parts=[Part(text=user_query)])

        async for _ in runner.run_async(
            user_id=USER_ID, session_id=current_session_id, new_message=initial_message
        ):
            pass

        session_state_data = await _session_service.get_session(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id
        )

        # Extract outputs from each step
        chart_type_info = session_state_data.state.get("chart_type_output")
        plotly_code = session_state_data.state.get("plotly_code_output")
        execution_summary = session_state_data.state.get("execution_summary")
        
        # Verify the artifact was saved
        final_artifact = await _artifact_service.load_artifact(
            app_name=APP_NAME, user_id=USER_ID, session_id=current_session_id, filename="plot.png"
        )

        print("✅ Visualization pipeline completed successfully.")
        return {
            "session_id": current_session_id,
            "app_name": APP_NAME,
            "chart_type_info": chart_type_info or "Not generated.",
            "generated_plotly_code": plotly_code or "Not generated.",
            "execution_summary": execution_summary or "Not executed.",
            "artifact_saved": "plot.png" if final_artifact else "No",
            "artifact_size_bytes": len(final_artifact.inline_data.data) if final_artifact else 0,
        }

    except Exception as e:
        print(f"❌ Pipeline failed with an error: {e}")
        traceback.print_exc()
        return {"error": str(e)}

# --- Example Usage ---
async def main():
    """Main function to demonstrate running the visualization pipeline."""
    print("--- Running Visualization Pipeline Agent ---")

    # Mock data representing the output of a prior SQL pipeline
    mock_sql_result = [
        {'product_name': 'T-Shirt', 'total_sold': 150},
        {'product_name': 'Jeans', 'total_sold': 120},
        {'product_name': 'Jacket', 'total_sold': 90},
        {'product_name': 'Socks', 'total_sold': 200},
        {'product_name': 'Hat', 'total_sold': 75},
    ]
    
    user_input = "Show me a bar chart of the top selling products"
    
    result = await execute_visualization_pipeline(user_input, mock_sql_result)

    print("\n--- VISUALIZATION PIPELINE RESULTS ---")
    if result.get("error"):
        print(f"Error: {result['error']}")
    else:
        # Pretty print the results
        for key, value in result.items():
            print(f"\n--- {key.replace('_', ' ').upper()} ---")
            if key == "generated_plotly_code":
                print(value)
            elif isinstance(value, (dict, list)):
                print(json.dumps(value, indent=2))
            else:
                print(value)
    print("------------------------------------")
    # You could also save the artifact to disk here to view it
    if not result.get("error") and result.get("artifact_saved") == "plot.png":
        print("\nAttempting to save artifact to disk...")
        try:
            session_id = result["session_id"]
            filename_to_load = "plot.png"

            # Load the artifact from the service using the session_id
            final_artifact = await _artifact_service.load_artifact(
                app_name=APP_NAME,
                user_id=USER_ID,
                session_id=session_id,
                filename=filename_to_load,
            )

            if final_artifact and final_artifact.inline_data:
                output_filename = "output_chart.png"
                # Open a file in binary write mode ('wb') and save the data
                with open(output_filename, "wb") as f:
                    f.write(final_artifact.inline_data.data)
                print(f"✅ Success! Chart saved to '{output_filename}'")
            else:
                print("⚠️ Could not retrieve the artifact from the service.")
        except Exception as e:
            print(f"❌ Failed to save artifact to disk. Error: {e}")


if __name__ == "__main__":
    asyncio.run(main())
File: app\services\visualization_agent\__init__.py
from . import agent
File: app\static\index.html
<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Serena Voice Assistant</title>
  <span id="connection-status">Connecting...</span>
  <script src="/static/js/app.js" type="module"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/static/css/styles.css">
</head>

<body>
  <header>
    <h1>Serena Voice Assistant</h1>
    <h3>Smart Engine for Rapid Exploration and Natural-language Anlaytics</h3>
    <p class="subtitle">Created by Ashish Johnson</p>
    <div class="status-indicator">
      <div class="status-item">
        <div id="status-dot" class="status-dot"></div>
        <span id="connection-status">Connecting...</span>
      </div>
      <div class="status-item" id="recording-container" style="display: none;">
        <div class="status-dot recording" style="animation: pulse-recording 1.5s infinite;"></div>
        <span id="recording-status">Recording</span>
      </div>
    </div>
  </header>
  <div class="chat-container">
    <div id="messages">
      <div id="typing-indicator" class="typing-indicator">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
    <form id="messageForm">
      <input type="text" id="message" name="message" placeholder="Type your message here..." autocomplete="off" />
      <button type="submit" id="sendButton" disabled>Send</button>
      <button type="button" id="startAudioButton">Enable Voice</button>
      <button type="button" id="stopAudioButton" style="display: none;">Stop Voice</button>
      <button type="button" id="toggleAgentVoiceButton">Agent Voice: ON</button>
    </form>
  </div>
</body>

</html>
File: app\static\css\styles.css
:root {
    --primary-color: #4285F4;
    --secondary-color: #34A853;
    --accent-color: #EA4335;
    --background-color: #F8F9FA;
    --text-color: #202124;
    --gray-light: #E8EAED;
    --gray-medium: #BDC1C6;
  }
  
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }
  
  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background-color: var(--background-color);
    color: var(--text-color);
    line-height: 1.6;
    padding: 20px;
    max-width: 800px;
    margin: 0 auto;
  }
  
  header {
    text-align: center;
    margin-bottom: 30px;
    padding-bottom: 20px;
    border-bottom: 1px solid var(--gray-light);
  }
  
  h1 {
    font-size: 24px;
    font-weight: 600;
    color: var(--primary-color);
    margin-bottom: 8px;
  }
  
  .subtitle {
    font-size: 14px;
    color: #5F6368;
  }
  
  .chat-container {
    background-color: white;
    border-radius: 12px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
    overflow: hidden;
    margin-bottom: 20px;
  }
  
  #messages {
    height: 400px;
    overflow-y: auto;
    padding: 20px;
    background-color: white;
    display: flex;
    flex-direction: column;
    scroll-behavior: smooth;
  }
  
  #messages p {
    margin-bottom: 16px;
    padding: 12px 16px;
    border-radius: 8px;
    max-width: 85%;
    word-wrap: break-word;
  }
  
  #messages p:last-child {
    margin-bottom: 0;
  }
  
  .agent-message {
    background-color: var(--gray-light);
    align-self: flex-start;
    border-bottom-left-radius: 2px;
    animation: fadeIn 0.3s ease-out;
  }
  
  /* Add a special style for messages that have audio */
  .audio-enabled .agent-message {
    border-left: 3px solid var(--secondary-color);
    padding-left: 14px;
  }
  
  /* Add a small audio icon for messages with audio */
  .audio-icon {
    display: inline-block;
    width: 18px;
    height: 18px;
    margin-right: 8px;
    vertical-align: middle;
    background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='%2334A853'%3E%3Cpath d='M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z'/%3E%3C/svg%3E");
    background-repeat: no-repeat;
    background-position: center;
    background-size: contain;
  }
  
  .user-message {
    background-color: var(--primary-color);
    color: white;
    align-self: flex-end;
    margin-left: auto;
    border-bottom-right-radius: 2px;
    animation: fadeIn 0.3s ease-out;
  }
  
  #messageForm {
    display: flex;
    gap: 10px;
    padding: 16px;
    background-color: white;
    border-top: 1px solid var(--gray-light);
  }
  
  #message {
    flex: 1;
    padding: 12px 16px;
    border: 1px solid var(--gray-medium);
    border-radius: 24px;
    font-size: 16px;
    outline: none;
    transition: border-color 0.2s ease;
  }
  
  #message:focus {
    border-color: var(--primary-color);
    box-shadow: 0 0 0 2px rgba(66, 133, 244, 0.3);
  }
  
  button {
    padding: 12px 20px;
    border: none;
    border-radius: 24px;
    font-size: 14px;
    font-weight: 500;
    cursor: pointer;
    transition: background-color 0.2s ease;
  }
  
  #sendButton {
    background-color: var(--primary-color);
    color: white;
  }
  
  #sendButton:disabled {
    background-color: var(--gray-medium);
    cursor: not-allowed;
  }
  
  #startAudioButton {
    background-color: var(--secondary-color);
    color: white;
  }
  
  #startAudioButton:disabled {
    background-color: var(--gray-medium);
    cursor: not-allowed;
  }
  
  #stopAudioButton {
    background-color: var(--accent-color);
    color: white;
    display: none;
  }
  
  .status-indicator {
    display: flex;
    align-items: center;
    justify-content: center;
    margin-top: 10px;
    font-size: 14px;
    color: #5F6368;
    gap: 20px;
  }
  
  .status-item {
    display: flex;
    align-items: center;
  }
  
  .status-dot {
    width: 8px;
    height: 8px;
    border-radius: 50%;
    background-color: var(--gray-medium);
    margin-right: 8px;
  }
  
  .status-dot.connected {
    background-color: var(--secondary-color);
  }
  
  .status-dot.recording {
    background-color: var(--accent-color);
  }
  
  .recording-active {
    position: relative;
  }
  
  @keyframes pulse-recording {
    0% {
      opacity: 1;
      transform: scale(1);
    }
  
    50% {
      opacity: 0.5;
      transform: scale(1.2);
    }
  
    100% {
      opacity: 1;
      transform: scale(1);
    }
  }
  
  @media (max-width: 600px) {
    body {
      padding: 10px;
    }
  
    #messages {
      height: 350px;
    }
  
    #messageForm {
      flex-direction: column;
    }
  
    button {
      width: 100%;
    }
  }
  
  /* Add animations */
  @keyframes fadeIn {
    from {
      opacity: 0;
      transform: translateY(10px);
    }
  
    to {
      opacity: 1;
      transform: translateY(0);
    }
  }
  
  @keyframes pulse {
    0% {
      transform: scale(1);
    }
  
    50% {
      transform: scale(1.05);
    }
  
    100% {
      transform: scale(1);
    }
  }
  
  button:hover:not(:disabled) {
    transform: translateY(-2px);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
  }
  
  button:active:not(:disabled) {
    transform: translateY(0);
  }
  
  #startAudioButton:hover:not(:disabled) {
    animation: pulse 1s infinite;
  }
  
  /* Add a typing indicator */
  .typing-indicator {
    display: flex;
    padding: 12px 16px;
    background-color: var(--gray-light);
    border-radius: 8px;
    width: fit-content;
    margin-bottom: 16px;
    align-self: flex-start;
    opacity: 0;
    transition: opacity 0.3s ease;
  }
  
  .typing-indicator.visible {
    opacity: 1;
  }
  
  .typing-indicator span {
    height: 8px;
    width: 8px;
    background-color: #70757A;
    border-radius: 50%;
    display: inline-block;
    margin: 0 2px;
  }
  
  .typing-indicator span:nth-child(1) {
    animation: bounce 1.2s infinite 0.2s;
  }
  
  .typing-indicator span:nth-child(2) {
    animation: bounce 1.2s infinite 0.4s;
  }
  
  .typing-indicator span:nth-child(3) {
    animation: bounce 1.2s infinite 0.6s;
  }
  
  @keyframes bounce {
    0%,
    100% {
      transform: translateY(0);
    }
  
    50% {
      transform: translateY(-5px);
    }
  }

  .chat-image {
    max-width: 350px; /* Set a max-width for the thumbnail in the chat */
    max-height: 300px; /* And a max-height */
    width: auto;       /* Maintain aspect ratio */
    height: auto;      /* Maintain aspect ratio */
    border-radius: 8px;
    margin-top: 8px;
    border: 1px solid var(--gray-light);
    display: block;
    cursor: zoom-in; /* Change cursor to indicate it's clickable */
    transition: transform 0.2s ease-in-out;
  }
  
  .chat-image:hover {
    transform: scale(1.03); /* Slight zoom effect on hover */
  }
  
  /* Optional styling for the caption above the image */
  .image-caption {
    margin-bottom: 0 !important; /* Override the default p margin inside the message */
    padding: 0 !important; /* Override padding */
    background-color: transparent !important; /* No separate background for the caption */
    font-style: italic;
    font-size: 0.9em;
    color: var(--text-color);
  }
  
  .modal {
    display: none; /* Hidden by default */
    position: fixed; /* Stay in place */
    z-index: 1000; /* Sit on top */
    padding-top: 60px; /* Location of the box */
    left: 0;
    top: 0;
    width: 100%; /* Full width */
    height: 100%; /* Full height */
    overflow: auto; /* Enable scroll if needed */
    background-color: rgb(0,0,0); /* Fallback color */
    background-color: rgba(0,0,0,0.9); /* Black w/ opacity */
    animation: fadeInModal 0.3s ease;
  }
  
  /* Modal Content (the image) */
  .modal-content {
    margin: auto;
    display: block;
    width: 80%;
    max-width: 900px;
    animation: zoomInModal 0.3s ease;
  }
  
  /* Caption of Modal Image */
  #modalCaption {
    margin: auto;
    display: block;
    width: 80%;
    max-width: 700px;
    text-align: center;
    color: #ccc;
    padding: 10px 0;
    height: 150px;
  }
  
  /* Add Animation */
  @keyframes zoomInModal {
    from {transform: scale(0.8); opacity: 0;}
    to {transform: scale(1); opacity: 1;}
  }
  @keyframes fadeInModal {
    from {opacity: 0;}
    to {opacity: 1;}
  }
  
  /* The Close Button */
  .close-modal-button {
    position: absolute;
    top: 15px;
    right: 35px;
    color: #f1f1f1;
    font-size: 40px;
    font-weight: bold;
    transition: 0.3s;
  }
  
  .close-modal-button:hover,
  .close-modal-button:focus {
    color: #bbb;
    text-decoration: none;
    cursor: pointer;
  }
  
  /* On smaller screens, where height is less than 800px, change the font-size of the close button */
  @media only screen and (max-width: 700px){
    .modal-content {
      width: 100%;
    }
  }
  
File: app\static\js\app.js
// Global variables
const sessionId = Math.random().toString().substring(10);
const ws_url = "ws://" + window.location.host + "/ws/" + sessionId;
let websocket = null;
let is_audio = false; // User input is audio
let agentWantsAudioOutput = true; // Agent output includes audio
let currentMessageId = null; // Track the current message ID during a conversation turn
let currentUserTranscriptionMessageId = null; // Track current USER transcription message ID #ashish

// Get DOM elements
const messageForm = document.getElementById("messageForm");
const messageInput = document.getElementById("message");
const messagesDiv = document.getElementById("messages");
const statusDot = document.getElementById("status-dot");
const connectionStatus = document.getElementById("connection-status");
const typingIndicator = document.getElementById("typing-indicator");
const startAudioButton = document.getElementById("startAudioButton");
const stopAudioButton = document.getElementById("stopAudioButton");
const recordingContainer = document.getElementById("recording-container");
const toggleAgentVoiceButton = document.getElementById("toggleAgentVoiceButton");

// WebSocket handlers
function connectWebsocket() {
  // If there's an existing websocket, close it before creating a new one
  if (websocket) {
    websocket.onclose = () => {}; // Prevent the default onclose handler from firing for this deliberate close
    websocket.close();
    websocket = null;
    console.log("Previous WebSocket connection closed for mode change.");
  }

  // Connect websocket
  const wsQuery = `?is_audio=${is_audio}&agent_wants_audio_output=${agentWantsAudioOutput}`;
  const sessionId = Math.random().toString().substring(10);
// Determine the correct protocol (wss for https, ws for http)
// window.location.protocol === 'https:' ? 'wss:' : 'ws:';
  const protocol = 'ws:' 
  const ws_url_base = `${protocol}//${window.location.host}/ws/${sessionId}`;
  const wsUrl = ws_url_base + wsQuery;
  console.log(`Connecting to WebSocket: ${wsUrl}`);
  websocket = new WebSocket(wsUrl);

  // Handle connection open
  websocket.onopen = function () {
    // Connection opened messages
    console.log("WebSocket connection opened.");
    connectionStatus.textContent = "Connected";
    statusDot.classList.add("connected");
    // Enable the Send button
    document.getElementById("sendButton").disabled = false;
    addSubmitHandler();
  };

  websocket.onmessage = function (event) {
    const message_from_server = JSON.parse(event.data);
    // console.log("[AGENT TO CLIENT] RAW: ", event.data); // For deep debugging if JSON parsing fails
    console.log("[AGENT TO CLIENT] Parsed: ", message_from_server); // General log for received message

    // --- 1. Handle User Transcription ---
    if (message_from_server.role === "user_transcription") {
      typingIndicator.classList.remove("visible"); // Agent isn't "typing" this
      const textData = message_from_server.data;
      let transcriptionElem = document.getElementById(currentUserTranscriptionMessageId);
      if (!transcriptionElem) { // First part of a new transcription
        const newTranscriptionId = "user-transc-" + Date.now() + Math.random().toString(36).substr(2, 5);
        transcriptionElem = document.createElement("p");
        transcriptionElem.id = newTranscriptionId;
        transcriptionElem.className = "user-message"; // Style like other user messages
        
        transcriptionElem.appendChild(document.createTextNode(textData)); // Append first chunk
        
        messagesDiv.appendChild(transcriptionElem);
        currentUserTranscriptionMessageId = newTranscriptionId;
      } else { // Subsequent part, append to existing element's text
        transcriptionElem.appendChild(document.createTextNode(textData)); // Append new chunk
      }
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
      return; // Processed user transcription, exit this handler invocation
    }

    // --- 2. Show Typing Indicator for Model's activity (if not turn complete) ---
    if (
      !message_from_server.turn_complete && // Must not be turn_complete
      message_from_server.role === "model" && // Only for model's own messages/audio
      (message_from_server.mime_type === "text/plain" || message_from_server.mime_type === "audio/pcm")
    ) {
      typingIndicator.classList.add("visible");
    }

    // --- 3. Handle Turn Completion ---
    if (message_from_server.turn_complete === true || message_from_server.interrupted === true) { // Check both
      currentMessageId = null; // Reset for agent's next text response
      currentUserTranscriptionMessageId = null; // Reset for user's next transcription
      typingIndicator.classList.remove("visible");
      console.log("[TURN] Complete or Interrupted.");
      return; // Processed turn_complete or interrupted, exit
    }

    // --- 4. Handle Agent Audio Output ---
    if (message_from_server.mime_type === "audio/pcm" && audioPlayerNode && message_from_server.role === "model" && agentWantsAudioOutput) {
      console.log("[AUDIO PLAYER] Received agent audio data. Current agent msg ID:", currentMessageId);
      audioPlayerNode.port.postMessage(base64ToArray(message_from_server.data));
      
      // Attempt to add audio icon to the corresponding text message, if it exists
      if (currentMessageId && agentWantsAudioOutput) { // Also check agentWantsAudioOutput here
        const messageElem = document.getElementById(currentMessageId);
        // Check if audio icon already exists to avoid duplicates if audio comes before all text
        if (messageElem && !messageElem.querySelector(".audio-icon")) { 
          const audioIcon = document.createElement("span");
          audioIcon.className = "audio-icon";
          // Prepend icon. Ensure there's a space if text is already there.
          if (messageElem.firstChild) {
            messageElem.insertBefore(audioIcon, messageElem.firstChild);
            messageElem.insertBefore(document.createTextNode(" "), audioIcon.nextSibling);
          } else {
            messageElem.appendChild(audioIcon);
            messageElem.appendChild(document.createTextNode(" "));
          }
        }
      }
    }

    if (message_from_server.mime_type === "image/png" && message_from_server.role === "model") {
      console.log("!!! DETECTED IMAGE MESSAGE !!!", message_from_server); 
      typingIndicator.classList.remove("visible"); // Hide indicator as content arrives
      
      const messageContainer = document.createElement("div");
      messageContainer.className = "agent-message"; // Use the same styling as agent text messages

      // Optional: Add a caption if provided
      if (message_from_server.caption) {
        const captionElem = document.createElement("p");
        captionElem.className = "image-caption";
        captionElem.textContent = message_from_server.caption;
        messageContainer.appendChild(captionElem);
      }
      
      const imageElem = document.createElement("img");
      imageElem.src = message_from_server.data; // The data is the URL
      imageElem.alt = "Generated Chart";
      imageElem.className = "chat-image";
      
      // Add onload handler to scroll after image has loaded and dimensions are known
      imageElem.onload = () => {
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      };

      messageContainer.appendChild(imageElem);
      messagesDiv.appendChild(messageContainer);
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }



    // --- 5. Handle Agent Text Output ---
    if (message_from_server.mime_type === "text/plain" && message_from_server.role === "model") {
      typingIndicator.classList.remove("visible"); // Hide indicator as text arrives
      let messageElem = document.getElementById(currentMessageId);
      if (!messageElem) { // First part of a new agent text message
        const newMessageId = "agent-msg-" + Date.now() + Math.random().toString(36).substr(2, 5);
        messageElem = document.createElement("p");
        messageElem.id = newMessageId;
        messageElem.className = "agent-message";
        
        if (agentWantsAudioOutput) { // If agent voice output is enabled, prepend an audio icon placeholder
          const audioIcon = document.createElement("span");
          audioIcon.className = "audio-icon";
          messageElem.appendChild(audioIcon);
          messageElem.appendChild(document.createTextNode(" ")); // Space after icon
        }
        
        messageElem.appendChild(document.createTextNode(message_from_server.data)); // Add first text chunk
        messagesDiv.appendChild(messageElem);
        currentMessageId = newMessageId; // Set current ID for subsequent appends
      } else { // Subsequent part, append to existing agent message element
        messageElem.appendChild(document.createTextNode(message_from_server.data)); // Append new text chunk
      }
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }
    
    // --- 6. Handle System Error Messages from Backend ---
    if (message_from_server.role === "system" && message_from_server.error) {
        console.error("Error from server:", message_from_server.error);
        const errorElem = document.createElement("p");
        errorElem.className = "error-message"; // You might want to style this class
        errorElem.textContent = "System Error: " + message_from_server.error;
        messagesDiv.appendChild(errorElem);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
        typingIndicator.classList.remove("visible");
        return;
    }
  }; // End of websocket.onmessage

  // Handle connection close
  websocket.onclose = function () {
    console.log("WebSocket connection closed.");
    document.getElementById("sendButton").disabled = true;
    connectionStatus.textContent = "Disconnected. Reconnecting...";
    statusDot.classList.remove("connected");
    typingIndicator.classList.remove("visible");
    // This onclose is for unexpected closes, try to reconnect
    setTimeout(function () {
      console.log("Reconnecting...");
      connectWebsocket();
    }, 5000);
  };

  websocket.onerror = function (e) {
    console.error("WebSocket error: ", e);
    connectionStatus.textContent = "Connection error";
    statusDot.classList.remove("connected");
    typingIndicator.classList.remove("visible");
  };
}

connectWebsocket(); // Initial connection

// Add submit handler to the form
function addSubmitHandler() {
  messageForm.onsubmit = function (e) {
    e.preventDefault();
    const message = messageInput.value;
    if (message) {
      const p = document.createElement("p");
      p.textContent = message;
      p.className = "user-message";
      messagesDiv.appendChild(p);
      messageInput.value = "";
      // Show typing indicator after sending message
      typingIndicator.classList.add("visible");
      sendMessage({
        mime_type: "text/plain",
        data: message,
        role: "user",
      });
      console.log("[CLIENT TO AGENT] " + message);
      // Scroll down to the bottom of the messagesDiv
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }
    return false;
  };
}

// Send a message to the server as a JSON string
function sendMessage(message) {
  if (websocket && websocket.readyState == WebSocket.OPEN) {
    const messageJson = JSON.stringify(message);
    websocket.send(messageJson);
  }
}

// Decode Base64 data to Array
function base64ToArray(base64) {
  const binaryString = window.atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes.buffer;
}

/**
 * Audio handling
 */
let audioPlayerNode;
let audioPlayerContext;
let audioRecorderNode;
let audioRecorderContext;
let micStream;
let isRecording = false;

// Import the audio worklets
import { startAudioPlayerWorklet } from "./audio-player.js";
import { startAudioRecorderWorklet } from "./audio-recorder.js";

// Start audio
function startAudio() {
  // Start audio output
  startAudioPlayerWorklet().then(([node, ctx]) => {
    audioPlayerNode = node;
    audioPlayerContext = ctx;
  });
  // Start audio input
  startAudioRecorderWorklet(audioRecorderHandler).then(
    ([node, ctx, stream]) => {
      audioRecorderNode = node;
      audioRecorderContext = ctx;
      micStream = stream;
      isRecording = true;
    }
  );
}

// Stop audio recording
function stopAudio() {
  if (audioRecorderNode) {
    audioRecorderNode.disconnect();
    audioRecorderNode = null;
  }
  if (audioRecorderContext) {
    audioRecorderContext
      .close()
      .catch((err) => console.error("Error closing audio context:", err));
    audioRecorderContext = null;
  }
  if (micStream) {
    micStream.getTracks().forEach((track) => track.stop());
    micStream = null;
  }
  isRecording = false;
}

// Start the audio only when the user clicked the button
// (due to the gesture requirement for the Web Audio API)
startAudioButton.addEventListener("click", () => {
  startAudioButton.disabled = true;
  startAudioButton.textContent = "Voice Enabled";
  startAudioButton.style.display = "none";
  stopAudioButton.style.display = "inline-block";
  recordingContainer.style.display = "flex";
  startAudio();
  is_audio = true; // User input is audio
  // Add class to messages container to enable audio styling (optional, for specific UI effects)
  // messagesDiv.classList.add("audio-enabled"); 
  connectWebsocket(); // reconnect with the new is_audio mode
});

// Stop audio recording when stop button is clicked
stopAudioButton.addEventListener("click", () => {
  stopAudio();
  stopAudioButton.style.display = "none";
  startAudioButton.style.display = "inline-block";
  startAudioButton.disabled = false;
  startAudioButton.textContent = "Enable Voice";
  recordingContainer.style.display = "none";
  // Remove audio styling class (optional)
  // messagesDiv.classList.remove("audio-enabled");
  is_audio = false; // User input is not audio
  connectWebsocket(); // reconnect with new is_audio mode
});

// Toggle agent voice output button
toggleAgentVoiceButton.addEventListener("click", () => {
  agentWantsAudioOutput = !agentWantsAudioOutput;
  toggleAgentVoiceButton.textContent = agentWantsAudioOutput ? "Agent Voice: ON" : "Agent Voice: OFF";
  connectWebsocket(); // Reconnect with the new agent voice output preference
});

// Audio recorder handler
function audioRecorderHandler(pcmData) {
  // Only send data if we're still recording
  if (!isRecording) return;
  // Send the pcm data as base64
  sendMessage({
    mime_type: "audio/pcm",
    data: arrayBufferToBase64(pcmData),
  });
  // Log every few samples to avoid flooding the console
  if (Math.random() < 0.01) {
    // Only log ~1% of audio chunks
    console.log("[CLIENT TO AGENT] sent audio data");
  }
}

// Encode an array buffer with Base64
function arrayBufferToBase64(buffer) {
  let binary = "";
  const bytes = new Uint8Array(buffer);
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return window.btoa(binary);
}
File: app\static\js\audio-player.js
/**
 * Audio Player Worklet
 */

export async function startAudioPlayerWorklet() {
  // 1. Create an AudioContext
  const audioContext = new AudioContext({
    sampleRate: 24000,
  });

  // 2. Load your custom processor code
  const workletURL = new URL("./pcm-player-processor.js", import.meta.url);
  await audioContext.audioWorklet.addModule(workletURL);

  // 3. Create an AudioWorkletNode
  const audioPlayerNode = new AudioWorkletNode(
    audioContext,
    "pcm-player-processor"
  );

  // 4. Connect to the destination
  audioPlayerNode.connect(audioContext.destination);

  // The audioPlayerNode.port is how we send messages (audio data) to the processor
  return [audioPlayerNode, audioContext];
}

File: app\static\js\audio-recorder.js
/**
 * Audio Recorder Worklet
 */

let micStream;

export async function startAudioRecorderWorklet(audioRecorderHandler) {
  // Create an AudioContext
  const audioRecorderContext = new AudioContext({ sampleRate: 16000 });
  console.log("AudioContext sample rate:", audioRecorderContext.sampleRate);

  // Load the AudioWorklet module
  const workletURL = new URL("./pcm-recorder-processor.js", import.meta.url);
  await audioRecorderContext.audioWorklet.addModule(workletURL);

  // Request access to the microphone
  micStream = await navigator.mediaDevices.getUserMedia({
    audio: { channelCount: 1 },
  });
  const source = audioRecorderContext.createMediaStreamSource(micStream);

  // Create an AudioWorkletNode that uses the PCMProcessor
  const audioRecorderNode = new AudioWorkletNode(
    audioRecorderContext,
    "pcm-recorder-processor"
  );

  // Connect the microphone source to the worklet.
  source.connect(audioRecorderNode);
  audioRecorderNode.port.onmessage = (event) => {
    // Convert to 16-bit PCM
    const pcmData = convertFloat32ToPCM(event.data);

    // Send the PCM data to the handler.
    audioRecorderHandler(pcmData);
  };
  return [audioRecorderNode, audioRecorderContext, micStream];
}

/**
 * Stop the microphone.
 */
export function stopMicrophone(micStream) {
  micStream.getTracks().forEach((track) => track.stop());
  console.log("stopMicrophone(): Microphone stopped.");
}

// Convert Float32 samples to 16-bit PCM.
function convertFloat32ToPCM(inputData) {
  // Create an Int16Array of the same length.
  const pcm16 = new Int16Array(inputData.length);
  for (let i = 0; i < inputData.length; i++) {
    // Multiply by 0x7fff (32767) to scale the float value to 16-bit PCM range.
    pcm16[i] = inputData[i] * 0x7fff;
  }
  // Return the underlying ArrayBuffer.
  return pcm16.buffer;
}

File: app\static\js\pcm-player-processor.js
/**
 * An audio worklet processor that stores the PCM audio data sent from the main thread
 * to a buffer and plays it.
 */
class PCMPlayerProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    // Init buffer
    this.bufferSize = 24000 * 180; // 24kHz x 180 seconds
    this.buffer = new Float32Array(this.bufferSize);
    this.writeIndex = 0;
    this.readIndex = 0;

    // Handle incoming messages from main thread
    this.port.onmessage = (event) => {
      // Reset the buffer when 'endOfAudio' message received
      if (event.data.command === "endOfAudio") {
        this.readIndex = this.writeIndex; // Clear the buffer
        console.log("endOfAudio received, clearing the buffer.");
        return;
      }

      // Decode the base64 data to int16 array.
      const int16Samples = new Int16Array(event.data);

      // Add the audio data to the buffer
      this._enqueue(int16Samples);
    };
  }

  // Push incoming Int16 data into our ring buffer.
  _enqueue(int16Samples) {
    for (let i = 0; i < int16Samples.length; i++) {
      // Convert 16-bit integer to float in [-1, 1]
      const floatVal = int16Samples[i] / 32768;

      // Store in ring buffer for left channel only (mono)
      this.buffer[this.writeIndex] = floatVal;
      this.writeIndex = (this.writeIndex + 1) % this.bufferSize;

      // Overflow handling (overwrite oldest samples)
      if (this.writeIndex === this.readIndex) {
        this.readIndex = (this.readIndex + 1) % this.bufferSize;
      }
    }
  }

  // The system calls `process()` ~128 samples at a time (depending on the browser).
  // We fill the output buffers from our ring buffer.
  process(inputs, outputs, parameters) {
    // Write a frame to the output
    const output = outputs[0];
    const framesPerBlock = output[0].length;
    for (let frame = 0; frame < framesPerBlock; frame++) {
      // Write the sample(s) into the output buffer
      output[0][frame] = this.buffer[this.readIndex]; // left channel
      if (output.length > 1) {
        output[1][frame] = this.buffer[this.readIndex]; // right channel
      }

      // Move the read index forward unless underflowing
      if (this.readIndex != this.writeIndex) {
        this.readIndex = (this.readIndex + 1) % this.bufferSize;
      }
    }

    // Returning true tells the system to keep the processor alive
    return true;
  }
}

registerProcessor("pcm-player-processor", PCMPlayerProcessor);

File: app\static\js\pcm-recorder-processor.js
class PCMProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    if (inputs.length > 0 && inputs[0].length > 0) {
      // Use the first channel
      const inputChannel = inputs[0][0];
      // Copy the buffer to avoid issues with recycled memory
      const inputCopy = new Float32Array(inputChannel);
      this.port.postMessage(inputCopy);
    }
    return true;
  }
}

registerProcessor("pcm-recorder-processor", PCMProcessor);

